{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Essay Writer\n",
    "\n",
    "This notebook follows closely the notebook developed by **Jeremy Chow** here:\n",
    "\n",
    "https://github.com/jeremyrchow/text-generation-kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Here the libraries we need for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Standard Data Science Libraries\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "# Neural Net Preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Neural Net Layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Neural Net Training\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "# The function below is from\n",
    "# https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr|et al)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Childhood epilepsy with centrotemporal spikes (CECTS, previously benign epilepsy with centrotemporal spikes, BECTS) is the most common idiopathic focal epilepsy, accounting for approximately 10% of all childhood onset epilepsies (Astradsson et al., 1998; Berg and Rychlik, 2015; Callenbach et al., 2010; Camfield and Camfield, 2014; Larsson and Eeg-Olofsson, 2006).\n",
      "This preliminary result suggests a disruption in the thalamocortical circuit in BECTS.\n"
     ]
    }
   ],
   "source": [
    "# Load text\n",
    "\n",
    "filename = 'Cat_and_Mark_papers.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# ... and split it into sentences.\n",
    "sentences = split_into_sentences(text)\n",
    "\n",
    "# Let's look at the first sentence,\n",
    "print(sentences[0])\n",
    "\n",
    "# ... and the last sentence,\n",
    "print(sentences[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Generally for NLP projects, to optimize the model's ability to gather meaning from the text, there would be removal of:\n",
    "- stop words such as _\"the\",\"a\",\"an\"_ \n",
    "- punctuation\n",
    "\n",
    "then tokenization (turning unique words into unique integers) of the text. However, because the goal here is to generate fluid and human-like speech, we want to preserve stop words. Instead we just use the Tokenizer method in the Keras library to perform the rest of the preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words in corpus using Keras Tokenizer.\n",
    "This function does the following:\n",
    "1. Removes punctation\n",
    "2. Sets all text to lower case\n",
    "3. Splits the words up, then assigns a unique integer to each word\n",
    "4. Replaces all instances of that word with the integer.\n",
    "\n",
    "Tokenization is necessary for preparing data for embedding layer (see model architecture section below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 50000 # Max size of the dictionary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the first sentence:\n",
      "Childhood epilepsy with centrotemporal spikes (CECTS, previously benign epilepsy with centrotemporal spikes, BECTS) is the most common idiopathic focal epilepsy, accounting for approximately 10% of all childhood onset epilepsies (Astradsson et al., 1998; Berg and Rychlik, 2015; Callenbach et al., 2010; Camfield and Camfield, 2014; Larsson and Eeg-Olofsson, 2006).\n",
      "It becomes a list of numbers:\n",
      "[178, 16, 9, 297, 22, 83, 560, 484, 16, 9, 297, 22, 17, 18, 1, 165, 158, 773, 34, 16, 1201, 15, 646, 132, 6, 133, 178, 202, 774, 961, 2, 3, 298, 427, 4, 1202, 90, 647, 2, 3, 69, 428, 4, 428, 41, 648, 4, 35, 962, 115]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'et': 2,\n",
       " 'al': 3,\n",
       " 'and': 4,\n",
       " 'in': 5,\n",
       " 'of': 6,\n",
       " 'to': 7,\n",
       " 'a': 8,\n",
       " 'with': 9,\n",
       " 'we': 10,\n",
       " 'that': 11,\n",
       " 'spike': 12,\n",
       " 'ripples': 13,\n",
       " 'will': 14,\n",
       " 'for': 15,\n",
       " 'epilepsy': 16,\n",
       " 'bects': 17,\n",
       " 'is': 18,\n",
       " 'this': 19,\n",
       " 'are': 20,\n",
       " 'ripple': 21,\n",
       " 'spikes': 22,\n",
       " 'these': 23,\n",
       " 'have': 24,\n",
       " '2019': 25,\n",
       " 'by': 26,\n",
       " 'from': 27,\n",
       " 'stimulation': 28,\n",
       " 'children': 29,\n",
       " 'on': 30,\n",
       " 'sleep': 31,\n",
       " 'as': 32,\n",
       " '2018': 33,\n",
       " 'focal': 34,\n",
       " 'eeg': 35,\n",
       " 'data': 36,\n",
       " 'spindles': 37,\n",
       " 'or': 38,\n",
       " '2017': 39,\n",
       " 'our': 40,\n",
       " '2014': 41,\n",
       " 'spindle': 42,\n",
       " 'automated': 43,\n",
       " 'detector': 44,\n",
       " 'white': 45,\n",
       " 'matter': 46,\n",
       " 'cognitive': 47,\n",
       " 'be': 48,\n",
       " '2016': 49,\n",
       " 'between': 50,\n",
       " 'rate': 51,\n",
       " '1': 52,\n",
       " 'an': 53,\n",
       " '2008': 54,\n",
       " '2011': 55,\n",
       " 'thalamocortical': 56,\n",
       " '2': 57,\n",
       " 'been': 58,\n",
       " '2012': 59,\n",
       " 'not': 60,\n",
       " 'patients': 61,\n",
       " 'seizures': 62,\n",
       " 'observed': 63,\n",
       " 'may': 64,\n",
       " 'activity': 65,\n",
       " 'model': 66,\n",
       " '2013': 67,\n",
       " 'models': 68,\n",
       " '2010': 69,\n",
       " 'during': 70,\n",
       " 'motor': 71,\n",
       " 'cortical': 72,\n",
       " 'clinical': 73,\n",
       " 'abnormalities': 74,\n",
       " 'cells': 75,\n",
       " 'events': 76,\n",
       " 'each': 77,\n",
       " 'interictal': 78,\n",
       " 'recordings': 79,\n",
       " 'e': 80,\n",
       " 'seizure': 81,\n",
       " 'jacobs': 82,\n",
       " 'cects': 83,\n",
       " 'deficits': 84,\n",
       " 'voltage': 85,\n",
       " 'neurons': 86,\n",
       " '3': 87,\n",
       " '0': 88,\n",
       " 'aim': 89,\n",
       " '2015': 90,\n",
       " 'has': 91,\n",
       " 'mechanisms': 92,\n",
       " 'epileptic': 93,\n",
       " 'both': 94,\n",
       " 'cortex': 95,\n",
       " 'performance': 96,\n",
       " 'high': 97,\n",
       " 'invasive': 98,\n",
       " 'brain': 99,\n",
       " 'ez': 100,\n",
       " 'parameters': 101,\n",
       " '2009': 102,\n",
       " 'fig': 103,\n",
       " 'disease': 104,\n",
       " 'specific': 105,\n",
       " 'kramer': 106,\n",
       " 'also': 107,\n",
       " 'g': 108,\n",
       " 'using': 109,\n",
       " 'human': 110,\n",
       " 'detection': 111,\n",
       " 'imaging': 112,\n",
       " 'work': 113,\n",
       " '2007': 114,\n",
       " '2006': 115,\n",
       " 'can': 116,\n",
       " 'computational': 117,\n",
       " 'provide': 118,\n",
       " 'approach': 119,\n",
       " 'identify': 120,\n",
       " 'i': 121,\n",
       " 'at': 122,\n",
       " '2004': 123,\n",
       " 'more': 124,\n",
       " 'fully': 125,\n",
       " '2005': 126,\n",
       " 'frequency': 127,\n",
       " 'single': 128,\n",
       " 'mouse': 129,\n",
       " 'electrical': 130,\n",
       " 'features': 131,\n",
       " '10': 132,\n",
       " 'all': 133,\n",
       " 'new': 134,\n",
       " 'memory': 135,\n",
       " 'then': 136,\n",
       " 'further': 137,\n",
       " 'consistent': 138,\n",
       " 'candidate': 139,\n",
       " 'neuron': 140,\n",
       " 'dynamics': 141,\n",
       " 'sensory': 142,\n",
       " 'gating': 143,\n",
       " 'huguenard': 144,\n",
       " 'found': 145,\n",
       " 'test': 146,\n",
       " 'biomarker': 147,\n",
       " 'which': 148,\n",
       " 'sensorimotor': 149,\n",
       " 'zone': 150,\n",
       " '2002': 151,\n",
       " 'circuit': 152,\n",
       " 'studies': 153,\n",
       " 'project': 154,\n",
       " 'occur': 155,\n",
       " '5': 156,\n",
       " 'one': 157,\n",
       " 'common': 158,\n",
       " 'observations': 159,\n",
       " 'hz': 160,\n",
       " 'well': 161,\n",
       " 'improved': 162,\n",
       " 'oscillations': 163,\n",
       " 'method': 164,\n",
       " 'most': 165,\n",
       " 'epileptiform': 166,\n",
       " 'prior': 167,\n",
       " 'if': 168,\n",
       " 'results': 169,\n",
       " 'dysfunction': 170,\n",
       " 'relationship': 171,\n",
       " 'subjects': 172,\n",
       " 'treatment': 173,\n",
       " 'surgical': 174,\n",
       " 'semi': 175,\n",
       " 'compared': 176,\n",
       " 'large': 177,\n",
       " 'childhood': 178,\n",
       " 'non': 179,\n",
       " 'but': 180,\n",
       " 'addition': 181,\n",
       " 'loop': 182,\n",
       " 'analysis': 183,\n",
       " 'resolution': 184,\n",
       " 'pyramidal': 185,\n",
       " 'excitatory': 186,\n",
       " 'n': 187,\n",
       " 'p': 188,\n",
       " 'expect': 189,\n",
       " 'characterized': 190,\n",
       " 'nrem': 191,\n",
       " 'across': 192,\n",
       " 'consolidation': 193,\n",
       " 'study': 194,\n",
       " 'than': 195,\n",
       " 'worrell': 196,\n",
       " 'fast': 197,\n",
       " 'animal': 198,\n",
       " 'perirolandic': 199,\n",
       " 'chu': 200,\n",
       " 'mice': 201,\n",
       " 'onset': 202,\n",
       " 'developmental': 203,\n",
       " 'developed': 204,\n",
       " 'same': 205,\n",
       " '6': 206,\n",
       " 'use': 207,\n",
       " 'traub': 208,\n",
       " 'scalp': 209,\n",
       " 'klink': 210,\n",
       " 'proposed': 211,\n",
       " 'density': 212,\n",
       " 'time': 213,\n",
       " 'two': 214,\n",
       " 'existing': 215,\n",
       " 'symptoms': 216,\n",
       " 'healthy': 217,\n",
       " 'active': 218,\n",
       " 'although': 219,\n",
       " 'detect': 220,\n",
       " 'different': 221,\n",
       " 'cell': 222,\n",
       " 'result': 223,\n",
       " 'however': 224,\n",
       " 'epileptogenic': 225,\n",
       " 'region': 226,\n",
       " 'patterns': 227,\n",
       " 'gliske': 228,\n",
       " 'current': 229,\n",
       " 'cnn': 230,\n",
       " 'parameter': 231,\n",
       " 'impact': 232,\n",
       " 'here': 233,\n",
       " 'through': 234,\n",
       " 'types': 235,\n",
       " 'pathologic': 236,\n",
       " 'identification': 237,\n",
       " 'identified': 238,\n",
       " 'explore': 239,\n",
       " 'first': 240,\n",
       " 'engel': 241,\n",
       " 'datasets': 242,\n",
       " 'increased': 243,\n",
       " 'determine': 244,\n",
       " 'min': 245,\n",
       " 'range': 246,\n",
       " 'circuits': 247,\n",
       " 'generate': 248,\n",
       " 'while': 249,\n",
       " 'encephalopathies': 250,\n",
       " 'ii': 251,\n",
       " 'evidence': 252,\n",
       " 'demonstrated': 253,\n",
       " 'dependent': 254,\n",
       " 'syndrome': 255,\n",
       " 'including': 256,\n",
       " 'biomarkers': 257,\n",
       " 'positive': 258,\n",
       " 'contribute': 259,\n",
       " 'co': 260,\n",
       " 'suggest': 261,\n",
       " 'interneurons': 262,\n",
       " 'do': 263,\n",
       " 'requires': 264,\n",
       " 'techniques': 265,\n",
       " 'controls': 266,\n",
       " 'vivo': 267,\n",
       " 'lfp': 268,\n",
       " 'regions': 269,\n",
       " 'fine': 270,\n",
       " 'processes': 271,\n",
       " 'shown': 272,\n",
       " 'walker': 273,\n",
       " '2003': 274,\n",
       " 'risk': 275,\n",
       " 'improve': 276,\n",
       " 'findings': 277,\n",
       " 'structural': 278,\n",
       " 'gabaergic': 279,\n",
       " 'available': 280,\n",
       " 'development': 281,\n",
       " 'van': 282,\n",
       " '50': 283,\n",
       " 'specificity': 284,\n",
       " 'experimental': 285,\n",
       " 'tissue': 286,\n",
       " 'previous': 287,\n",
       " 'deep': 288,\n",
       " 'duration': 289,\n",
       " 'maturation': 290,\n",
       " 'whether': 291,\n",
       " 'lundstrom': 292,\n",
       " 'channels': 293,\n",
       " 'spiking': 294,\n",
       " 'neural': 295,\n",
       " 'mst': 296,\n",
       " 'centrotemporal': 297,\n",
       " '1998': 298,\n",
       " 'beenhakker': 299,\n",
       " 'support': 300,\n",
       " 'present': 301,\n",
       " 'thalamic': 302,\n",
       " '2001': 303,\n",
       " 'after': 304,\n",
       " 'electrodes': 305,\n",
       " 'reported': 306,\n",
       " 'thus': 307,\n",
       " 'there': 308,\n",
       " 'interventions': 309,\n",
       " 'bragin': 310,\n",
       " 'low': 311,\n",
       " 'inhibitory': 312,\n",
       " 'physiological': 313,\n",
       " 'u': 314,\n",
       " 'neurostimulation': 315,\n",
       " 'aims': 316,\n",
       " 'rns': 317,\n",
       " 'preliminary': 318,\n",
       " 'meg': 319,\n",
       " 'rapid': 320,\n",
       " '1997': 321,\n",
       " 'pathological': 322,\n",
       " 'function': 323,\n",
       " 'related': 324,\n",
       " 'required': 325,\n",
       " 'would': 326,\n",
       " 'novel': 327,\n",
       " 'control': 328,\n",
       " 'understanding': 329,\n",
       " 'their': 330,\n",
       " 'von': 331,\n",
       " 'humans': 332,\n",
       " 'populations': 333,\n",
       " 'age': 334,\n",
       " 'find': 335,\n",
       " 'reflect': 336,\n",
       " '1999': 337,\n",
       " 'electrode': 338,\n",
       " 'discharges': 339,\n",
       " 'number': 340,\n",
       " 'propose': 341,\n",
       " 'standard': 342,\n",
       " 'was': 343,\n",
       " 'microstructure': 344,\n",
       " 'task': 345,\n",
       " 'connectivity': 346,\n",
       " 'neurosurgical': 347,\n",
       " 'address': 348,\n",
       " 'network': 349,\n",
       " '2000': 350,\n",
       " 'wang': 351,\n",
       " 'per': 352,\n",
       " 'abnormal': 353,\n",
       " '15': 354,\n",
       " 'generated': 355,\n",
       " 'synaptic': 356,\n",
       " 'long': 357,\n",
       " 'correlate': 358,\n",
       " 'neuropsychological': 359,\n",
       " 'limited': 360,\n",
       " 'other': 361,\n",
       " 'spatially': 362,\n",
       " 'over': 363,\n",
       " 'develop': 364,\n",
       " 'continuous': 365,\n",
       " 'no': 366,\n",
       " 'within': 367,\n",
       " 'potential': 368,\n",
       " 'gotman': 369,\n",
       " 'intracranial': 370,\n",
       " 'detected': 371,\n",
       " 'ms': 372,\n",
       " 'amplitude': 373,\n",
       " 'used': 374,\n",
       " 'such': 375,\n",
       " 'focus': 376,\n",
       " 'fibers': 377,\n",
       " 'process': 378,\n",
       " 'disorder': 379,\n",
       " 'years': 380,\n",
       " 'source': 381,\n",
       " 'expertise': 382,\n",
       " 'approaches': 383,\n",
       " 'outcome': 384,\n",
       " 'apply': 385,\n",
       " 'produce': 386,\n",
       " 'additional': 387,\n",
       " 'layer': 388,\n",
       " 'disruption': 389,\n",
       " 'mechanism': 390,\n",
       " 'functional': 391,\n",
       " 'normal': 392,\n",
       " 'compare': 393,\n",
       " 'target': 394,\n",
       " 'show': 395,\n",
       " 'because': 396,\n",
       " 'fogerson': 397,\n",
       " 'de': 398,\n",
       " 'values': 399,\n",
       " '2020': 400,\n",
       " 'small': 401,\n",
       " 'patient': 402,\n",
       " 'kobayashi': 403,\n",
       " 'correlates': 404,\n",
       " 'staba': 405,\n",
       " 'minutes': 406,\n",
       " 'false': 407,\n",
       " 'value': 408,\n",
       " 'directly': 409,\n",
       " 'changes': 410,\n",
       " 'less': 411,\n",
       " 'reduce': 412,\n",
       " 'microstructural': 413,\n",
       " 'could': 414,\n",
       " 'hypothesis': 415,\n",
       " 'corresponding': 416,\n",
       " 'alone': 417,\n",
       " 'analyze': 418,\n",
       " 'outcomes': 419,\n",
       " 'type': 420,\n",
       " 'so': 421,\n",
       " 'closed': 422,\n",
       " 'power': 423,\n",
       " 'simulate': 424,\n",
       " 'ib': 425,\n",
       " 'impaired': 426,\n",
       " 'berg': 427,\n",
       " 'camfield': 428,\n",
       " 'activated': 429,\n",
       " 'severity': 430,\n",
       " 'sigma': 431,\n",
       " 'established': 432,\n",
       " 'application': 433,\n",
       " 'feature': 434,\n",
       " 'several': 435,\n",
       " 'kim': 436,\n",
       " 'rhythms': 437,\n",
       " 'represent': 438,\n",
       " 'deficit': 439,\n",
       " 'those': 440,\n",
       " 'separate': 441,\n",
       " 'testing': 442,\n",
       " 'measure': 443,\n",
       " 'resolving': 444,\n",
       " 'include': 445,\n",
       " 'who': 446,\n",
       " 'direct': 447,\n",
       " '80': 448,\n",
       " 'manually': 449,\n",
       " 'up': 450,\n",
       " 'how': 451,\n",
       " '4': 452,\n",
       " 'manual': 453,\n",
       " 'predictive': 454,\n",
       " 'potentials': 455,\n",
       " 'gap': 456,\n",
       " 'when': 457,\n",
       " 'neuronal': 458,\n",
       " 'detections': 459,\n",
       " 'goal': 460,\n",
       " 'spatial': 461,\n",
       " 'evaluate': 462,\n",
       " 'complete': 463,\n",
       " 'targeting': 464,\n",
       " 'successful': 465,\n",
       " 'accurate': 466,\n",
       " 'optimal': 467,\n",
       " 'team': 468,\n",
       " 'disrupt': 469,\n",
       " 'following': 470,\n",
       " '1992': 471,\n",
       " 'rapidly': 472,\n",
       " 'inhibition': 473,\n",
       " 'roopun': 474,\n",
       " 'recording': 475,\n",
       " 'individual': 476,\n",
       " '–': 477,\n",
       " 'open': 478,\n",
       " 'learning': 479,\n",
       " '7': 480,\n",
       " 'cre': 481,\n",
       " 'attention': 482,\n",
       " 'trn': 483,\n",
       " 'benign': 484,\n",
       " 'encephalopathy': 485,\n",
       " 'general': 486,\n",
       " 'unknown': 487,\n",
       " 'pathophysiology': 488,\n",
       " 'brief': 489,\n",
       " 'similar': 490,\n",
       " 'andersen': 491,\n",
       " '1967': 492,\n",
       " 'clemente': 493,\n",
       " 'perez': 494,\n",
       " 'steriade': 495,\n",
       " 'known': 496,\n",
       " 'efforts': 497,\n",
       " 'smith': 498,\n",
       " 'nishida': 499,\n",
       " 'shared': 500,\n",
       " 'circuitry': 501,\n",
       " 'detectors': 502,\n",
       " 'accurately': 503,\n",
       " 'manoach': 504,\n",
       " 'severe': 505,\n",
       " 'glutamatergic': 506,\n",
       " 'did': 507,\n",
       " 'limitations': 508,\n",
       " 'day': 509,\n",
       " 'future': 510,\n",
       " 'wave': 511,\n",
       " 'networks': 512,\n",
       " '200': 513,\n",
       " 'localize': 514,\n",
       " 'andrade': 515,\n",
       " 'second': 516,\n",
       " 'better': 517,\n",
       " 'population': 518,\n",
       " 'based': 519,\n",
       " 'only': 520,\n",
       " 'simon': 521,\n",
       " 'underlying': 522,\n",
       " 'remain': 523,\n",
       " '100': 524,\n",
       " 'were': 525,\n",
       " 'note': 526,\n",
       " 'simulated': 527,\n",
       " 'sensitivity': 528,\n",
       " 'fractional': 529,\n",
       " 'anisotropy': 530,\n",
       " 'decrease': 531,\n",
       " 'remains': 532,\n",
       " 'multiple': 533,\n",
       " 'adjacent': 534,\n",
       " 'rhythm': 535,\n",
       " 'intervention': 536,\n",
       " 'interdisciplinary': 537,\n",
       " 'research': 538,\n",
       " 'refractory': 539,\n",
       " 'guignard': 540,\n",
       " 'event': 541,\n",
       " 'channel': 542,\n",
       " 'optical': 543,\n",
       " 'cellular': 544,\n",
       " 'silico': 545,\n",
       " 'optimized': 546,\n",
       " 'mgh': 547,\n",
       " 'training': 548,\n",
       " 'see': 549,\n",
       " '36': 550,\n",
       " 'fail': 551,\n",
       " 'column': 552,\n",
       " 'rs': 553,\n",
       " 'firing': 554,\n",
       " '30': 555,\n",
       " 'pilocarpine': 556,\n",
       " 'auditory': 557,\n",
       " 'csws': 558,\n",
       " 'nap': 559,\n",
       " 'previously': 560,\n",
       " 'clinically': 561,\n",
       " 'now': 562,\n",
       " 'despite': 563,\n",
       " 'electrophysiological': 564,\n",
       " 'suggests': 565,\n",
       " 'band': 566,\n",
       " 'disruptions': 567,\n",
       " 'generation': 568,\n",
       " 'link': 569,\n",
       " 'therefore': 570,\n",
       " 'many': 571,\n",
       " 'due': 572,\n",
       " 'syndromes': 573,\n",
       " 'thalamus': 574,\n",
       " 'ciumas': 575,\n",
       " 'diffuse': 576,\n",
       " 'utilize': 577,\n",
       " 'vitro': 578,\n",
       " '1993': 579,\n",
       " 'measured': 580,\n",
       " 'distinct': 581,\n",
       " 'either': 582,\n",
       " 'increase': 583,\n",
       " 'poor': 584,\n",
       " 'given': 585,\n",
       " 'measures': 586,\n",
       " 'cases': 587,\n",
       " 'validated': 588,\n",
       " 'strategies': 589,\n",
       " 'validation': 590,\n",
       " 'offer': 591,\n",
       " 'groups': 592,\n",
       " 'valenca': 593,\n",
       " 'it': 594,\n",
       " 'expert': 595,\n",
       " 'artifacts': 596,\n",
       " 'ellenrieder': 597,\n",
       " 'larger': 598,\n",
       " 'urrestarazu': 599,\n",
       " 'understood': 600,\n",
       " 'junctions': 601,\n",
       " 'appear': 602,\n",
       " 'applied': 603,\n",
       " 'sufficient': 604,\n",
       " 'best': 605,\n",
       " 'tested': 606,\n",
       " 'longer': 607,\n",
       " 'showed': 608,\n",
       " 'utility': 609,\n",
       " 'predict': 610,\n",
       " 'they': 611,\n",
       " 'association': 612,\n",
       " 'gender': 613,\n",
       " 'diffusion': 614,\n",
       " 'mean': 615,\n",
       " 'decreased': 616,\n",
       " 'differences': 617,\n",
       " 'suggesting': 618,\n",
       " 'comorbidities': 619,\n",
       " 'fundamental': 620,\n",
       " 's': 621,\n",
       " 'reduced': 622,\n",
       " 'recently': 623,\n",
       " 'option': 624,\n",
       " 'critical': 625,\n",
       " 'generating': 626,\n",
       " 'consuming': 627,\n",
       " 'three': 628,\n",
       " 'modeling': 629,\n",
       " 'observe': 630,\n",
       " 'combined': 631,\n",
       " 'interneuron': 632,\n",
       " 'temporal': 633,\n",
       " 'biophysical': 634,\n",
       " 'jobst': 635,\n",
       " 'eden': 636,\n",
       " 'han': 637,\n",
       " 'train': 638,\n",
       " 'extend': 639,\n",
       " 'symmetry': 640,\n",
       " 'beta': 641,\n",
       " 'basket': 642,\n",
       " 'settings': 643,\n",
       " 'effect': 644,\n",
       " 'disruptive': 645,\n",
       " 'approximately': 646,\n",
       " 'callenbach': 647,\n",
       " 'larsson': 648,\n",
       " 'arising': 649,\n",
       " 'abundant': 650,\n",
       " 'difficulties': 651,\n",
       " 'coordination': 652,\n",
       " 'processing': 653,\n",
       " 'relate': 654,\n",
       " 'detailed': 655,\n",
       " 'electrophysiology': 656,\n",
       " 'mccormick': 657,\n",
       " 'williams': 658,\n",
       " '1953': 659,\n",
       " 'sánchez': 660,\n",
       " 'fernández': 661,\n",
       " 'role': 662,\n",
       " 'hahn': 663,\n",
       " 'pathophysiological': 664,\n",
       " 'iii': 665,\n",
       " 'methods': 666,\n",
       " 'introduce': 667,\n",
       " 'fisher': 668,\n",
       " 'state': 669,\n",
       " 'xiao': 670,\n",
       " 'primarily': 671,\n",
       " 'areas': 672,\n",
       " 'recorded': 673,\n",
       " 'short': 674,\n",
       " 'sensitive': 675,\n",
       " 'cross': 676,\n",
       " 'transient': 677,\n",
       " 'require': 678,\n",
       " 'concurrent': 679,\n",
       " 'genetic': 680,\n",
       " 'case': 681,\n",
       " 'provides': 682,\n",
       " 'localization': 683,\n",
       " 'course': 684,\n",
       " 'evaluating': 685,\n",
       " 'review': 686,\n",
       " 'majority': 687,\n",
       " 'procedure': 688,\n",
       " 'coupled': 689,\n",
       " 'pattern': 690,\n",
       " 'above': 691,\n",
       " 'reliably': 692,\n",
       " 'visual': 693,\n",
       " 'had': 694,\n",
       " 'seen': 695,\n",
       " 'oscillation': 696,\n",
       " 'without': 697,\n",
       " 'finally': 698,\n",
       " 'where': 699,\n",
       " 'challenge': 700,\n",
       " 'still': 701,\n",
       " 'subjective': 702,\n",
       " 'reliability': 703,\n",
       " 'investigate': 704,\n",
       " 'often': 705,\n",
       " 'boys': 706,\n",
       " 'sequence': 707,\n",
       " 'local': 708,\n",
       " 'fiber': 709,\n",
       " 'properties': 710,\n",
       " '20': 711,\n",
       " 'delay': 712,\n",
       " 'suggested': 713,\n",
       " 'recent': 714,\n",
       " 'unique': 715,\n",
       " 'knowledge': 716,\n",
       " 'responsible': 717,\n",
       " 'relative': 718,\n",
       " 'free': 719,\n",
       " 'constrained': 720,\n",
       " 'hand': 721,\n",
       " 'maximal': 722,\n",
       " 'effective': 723,\n",
       " 'treated': 724,\n",
       " 'combines': 725,\n",
       " 'experiments': 726,\n",
       " 'effects': 727,\n",
       " 'subset': 728,\n",
       " 'medication': 729,\n",
       " 'surgery': 730,\n",
       " 'jr': 731,\n",
       " 'cohen': 732,\n",
       " 'curtis': 733,\n",
       " 'weiss': 734,\n",
       " 'sabolek': 735,\n",
       " 'frauscher': 736,\n",
       " 'failure': 737,\n",
       " 'cimbalnik': 738,\n",
       " 'reliable': 739,\n",
       " 'experts': 740,\n",
       " 'alvarado': 741,\n",
       " 'rojas': 742,\n",
       " 'gaba': 743,\n",
       " 'image': 744,\n",
       " 'set': 745,\n",
       " 'action': 746,\n",
       " 'sisterson': 747,\n",
       " 'classified': 748,\n",
       " 'spectrogram': 749,\n",
       " 'spectrograms': 750,\n",
       " 'undergo': 751,\n",
       " 'step': 752,\n",
       " 'compute': 753,\n",
       " 'difference': 754,\n",
       " 'allow': 755,\n",
       " 'somarchon': 756,\n",
       " 'subthreshold': 757,\n",
       " 'gamma': 758,\n",
       " '40': 759,\n",
       " 'compartment': 760,\n",
       " 'ng': 761,\n",
       " 'connects': 762,\n",
       " 'burst': 763,\n",
       " 'followed': 764,\n",
       " 'stimulus': 765,\n",
       " 'response': 766,\n",
       " 'lks': 767,\n",
       " 'deficient': 768,\n",
       " 'diazepam': 769,\n",
       " 'increases': 770,\n",
       " 'experiment': 771,\n",
       " 'participants': 772,\n",
       " 'idiopathic': 773,\n",
       " 'epilepsies': 774,\n",
       " 'ross': 775,\n",
       " 'group': 776,\n",
       " 'among': 777,\n",
       " 'information': 778,\n",
       " 'bal': 779,\n",
       " 'implicated': 780,\n",
       " 'generalized': 781,\n",
       " 'bursts': 782,\n",
       " 'underlie': 783,\n",
       " 'contrast': 784,\n",
       " 'neurocognitive': 785,\n",
       " 'fogel': 786,\n",
       " 'peters': 787,\n",
       " 'rasch': 788,\n",
       " 'rosanova': 789,\n",
       " 'ulrich': 790,\n",
       " 'siapas': 791,\n",
       " 'wilson': 792,\n",
       " 'sirota': 793,\n",
       " 'tamaki': 794,\n",
       " 'states': 795,\n",
       " 'resulting': 796,\n",
       " 'mechanistic': 797,\n",
       " 'warby': 798,\n",
       " 'validate': 799,\n",
       " 'setting': 800,\n",
       " 'stages': 801,\n",
       " 'opportunity': 802,\n",
       " 'opportunities': 803,\n",
       " 'overlap': 804,\n",
       " 'genetically': 805,\n",
       " 'ostrowski': 806,\n",
       " 'collection': 807,\n",
       " 'fuentealba': 808,\n",
       " 'pharmacologic': 809,\n",
       " 'treatments': 810,\n",
       " '1994': 811,\n",
       " 'phase': 812,\n",
       " 'promote': 813,\n",
       " 'period': 814,\n",
       " 'associated': 815,\n",
       " 'language': 816,\n",
       " 'included': 817,\n",
       " 'longitudinal': 818,\n",
       " 'permanent': 819,\n",
       " 'supporting': 820,\n",
       " 'importantly': 821,\n",
       " 'currently': 822,\n",
       " 'proven': 823,\n",
       " 'treat': 824,\n",
       " 'conversely': 825,\n",
       " 'leminen': 826,\n",
       " 'promising': 827,\n",
       " 'occurrence': 828,\n",
       " 'threshold': 829,\n",
       " 'signals': 830,\n",
       " 'promise': 831,\n",
       " 'buzsaki': 832,\n",
       " 'hours': 833,\n",
       " 'significantly': 834,\n",
       " 'selective': 835,\n",
       " 'ellenreider': 836,\n",
       " 'performs': 837,\n",
       " 'alternative': 838,\n",
       " 'j': 839,\n",
       " 'unclear': 840,\n",
       " 'sampling': 841,\n",
       " 'interest': 842,\n",
       " 'literature': 843,\n",
       " 'procedures': 844,\n",
       " 'expected': 845,\n",
       " 'subsequent': 846,\n",
       " 'sets': 847,\n",
       " 'reduction': 848,\n",
       " 'rater': 849,\n",
       " 'exploration': 850,\n",
       " 'essential': 851,\n",
       " 'wide': 852,\n",
       " 'presentation': 853,\n",
       " 'characteristics': 854,\n",
       " 'grooved': 855,\n",
       " 'pegboard': 856,\n",
       " 'multimodal': 857,\n",
       " 'mri': 858,\n",
       " 'radial': 859,\n",
       " 'diffusivity': 860,\n",
       " 'outside': 861,\n",
       " 'experience': 862,\n",
       " 'school': 863,\n",
       " 'poorly': 864,\n",
       " 'stereotyped': 865,\n",
       " 'localized': 866,\n",
       " 'male': 867,\n",
       " 'likely': 868,\n",
       " 'focused': 869,\n",
       " 'receptor': 870,\n",
       " '1975': 871,\n",
       " 'identifying': 872,\n",
       " 'correspond': 873,\n",
       " 'relates': 874,\n",
       " 'nearly': 875,\n",
       " 'third': 876,\n",
       " 'fails': 877,\n",
       " 'manifest': 878,\n",
       " 'assess': 879,\n",
       " 'pipeline': 880,\n",
       " 'commonly': 881,\n",
       " 'significant': 882,\n",
       " 'principled': 883,\n",
       " 'us': 884,\n",
       " 'care': 885,\n",
       " 'record': 886,\n",
       " 'defined': 887,\n",
       " '2009a': 888,\n",
       " 'avoli': 889,\n",
       " 'cm2': 890,\n",
       " 'kinoshita': 891,\n",
       " 'kossoff': 892,\n",
       " 'velasco': 893,\n",
       " 'yamamoto': 894,\n",
       " 'buzsáki': 895,\n",
       " 'jiruska': 896,\n",
       " 'focusing': 897,\n",
       " 'analyzed': 898,\n",
       " 'collected': 899,\n",
       " 'agreement': 900,\n",
       " 'convolutional': 901,\n",
       " 'tools': 902,\n",
       " 'probe': 903,\n",
       " 'competing': 904,\n",
       " 'conventional': 905,\n",
       " 'calcium': 906,\n",
       " 'intracellular': 907,\n",
       " 'membrane': 908,\n",
       " 'technique': 909,\n",
       " 'piatkevich': 910,\n",
       " 'field': 911,\n",
       " 'driving': 912,\n",
       " 'currents': 913,\n",
       " 'system': 914,\n",
       " 'trial': 915,\n",
       " 'trials': 916,\n",
       " 'neuromodulation': 917,\n",
       " 'error': 918,\n",
       " 'post': 919,\n",
       " 'interval': 920,\n",
       " 'statistical': 921,\n",
       " 'richardson': 922,\n",
       " 'bu': 923,\n",
       " 'kopell': 924,\n",
       " 'strong': 925,\n",
       " 'establish': 926,\n",
       " 'institutions': 927,\n",
       " 'initial': 928,\n",
       " 'probability': 929,\n",
       " 'r01': 930,\n",
       " 'rem': 931,\n",
       " 'perform': 932,\n",
       " 'month': 933,\n",
       " 'cycle': 934,\n",
       " 'change': 935,\n",
       " 'consider': 936,\n",
       " 'depend': 937,\n",
       " 'köhling': 938,\n",
       " 'about': 939,\n",
       " 'consists': 940,\n",
       " 'lts': 941,\n",
       " 'synapses': 942,\n",
       " 'drive': 943,\n",
       " 'final': 944,\n",
       " 'proposal': 945,\n",
       " 'described': 946,\n",
       " 'pv': 947,\n",
       " 'som': 948,\n",
       " 'implanted': 949,\n",
       " 'mm': 950,\n",
       " 'animals': 951,\n",
       " 'any': 952,\n",
       " 'slow': 953,\n",
       " 'stimuli': 954,\n",
       " 'hour': 955,\n",
       " 'characteristic': 956,\n",
       " 'wamsley': 957,\n",
       " 'ngo': 958,\n",
       " 'central': 959,\n",
       " '11': 960,\n",
       " 'astradsson': 961,\n",
       " 'olofsson': 962,\n",
       " 'left': 963,\n",
       " 'disorders': 964,\n",
       " 'behavioral': 965,\n",
       " 'noted': 966,\n",
       " 'scheffer': 967,\n",
       " 'resolve': 968,\n",
       " 'ages': 969,\n",
       " 'adolescence': 970,\n",
       " 'oscillatory': 971,\n",
       " '9': 972,\n",
       " 'prominent': 973,\n",
       " 'bjørnæs': 974,\n",
       " 'vega': 975,\n",
       " 'term': 976,\n",
       " 'farmer': 977,\n",
       " 'demonstrate': 978,\n",
       " 'indicate': 979,\n",
       " 'quantify': 980,\n",
       " 'tasks': 981,\n",
       " 'jones': 982,\n",
       " 'leading': 983,\n",
       " 'gao': 984,\n",
       " 'kessi': 985,\n",
       " 'lemke': 986,\n",
       " 'strehlow': 987,\n",
       " 'broader': 988,\n",
       " 'rare': 989,\n",
       " 'reticular': 990,\n",
       " 'nucleus': 991,\n",
       " 'like': 992,\n",
       " 'nuclei': 993,\n",
       " 'propagated': 994,\n",
       " 'depth': 995,\n",
       " 'absence': 996,\n",
       " 'particular': 997,\n",
       " 'tied': 998,\n",
       " 'discrete': 999,\n",
       " 'typically': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's the first sentence,\n",
    "print(\"Here's the first sentence:\")\n",
    "print(sentences[0])\n",
    "\n",
    "# It becomes a list of numbers,\n",
    "print(\"It becomes a list of numbers:\")\n",
    "print(sequences[0])\n",
    "\n",
    "# And here's the dictionary that linkss each number to a word,\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size in this corpus:  2899\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
    "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print('Vocabulary size in this corpus: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on 19 words to predict the 20th\n",
    "sentence_len = 20\n",
    "pred_len     = 1\n",
    "train_len    = sentence_len - pred_len\n",
    "seq = []\n",
    "\n",
    "# Sliding window to generate train data\n",
    "for i in range(len(text)-sentence_len):\n",
    "    seq.append(text[i:i+sentence_len])\n",
    "# Reverse dictionary to decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row in seq is a 20 word long window. We append he first 19 words as the input to predict the 20th word\n",
    "trainX = []\n",
    "trainy = []\n",
    "for i in seq:\n",
    "    trainX.append(i[:train_len])\n",
    "    trainy.append(i[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture:\n",
    "1. Embedding layer\n",
    "    - Helps model understand 'meaning' of words by mapping them to representative vector space instead of semantic integers\n",
    "2. Stacked LSTM layers\n",
    "    - Stacked LSTMs add more depth than additional cells in a single LSTM layer (see paper: https://arxiv.org/abs/1303.5778)\n",
    "    - The first LSTM layer must have `return sequences` flag set to True in order to pass sequence information to the second LSTM layer instead of just its end states\n",
    "3. Dense (regression) layer with ReLU activation\n",
    "4. Dense layer with Softmax activation \n",
    "    - Outputs word probability across entire vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
    "    LSTM(150, return_sequences=True),\n",
    "    LSTM(150),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And teach it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 6.7640 - acc: 0.0319\n",
      "Epoch 00001: loss improved from inf to 6.76415, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 651us/sample - loss: 6.7642 - acc: 0.0321\n",
      "Epoch 2/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 6.3139 - acc: 0.0496\n",
      "Epoch 00002: loss improved from 6.76415 to 6.31425, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 13s 598us/sample - loss: 6.3142 - acc: 0.0496\n",
      "Epoch 3/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 6.0376 - acc: 0.0929\n",
      "Epoch 00003: loss improved from 6.31425 to 6.03820, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 634us/sample - loss: 6.0382 - acc: 0.0929\n",
      "Epoch 4/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 5.7761 - acc: 0.1091\n",
      "Epoch 00004: loss improved from 6.03820 to 5.77745, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 637us/sample - loss: 5.7775 - acc: 0.1089\n",
      "Epoch 5/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 5.5884 - acc: 0.1206\n",
      "Epoch 00005: loss improved from 5.77745 to 5.58747, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 649us/sample - loss: 5.5875 - acc: 0.1207\n",
      "Epoch 6/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 5.4546 - acc: 0.1290\n",
      "Epoch 00006: loss improved from 5.58747 to 5.45468, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 665us/sample - loss: 5.4547 - acc: 0.1289\n",
      "Epoch 7/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 5.3514 - acc: 0.1358\n",
      "Epoch 00007: loss improved from 5.45468 to 5.35131, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 668us/sample - loss: 5.3513 - acc: 0.1357\n",
      "Epoch 8/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 5.2569 - acc: 0.1425\n",
      "Epoch 00008: loss improved from 5.35131 to 5.25666, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 676us/sample - loss: 5.2567 - acc: 0.1426\n",
      "Epoch 9/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 5.1721 - acc: 0.1489\n",
      "Epoch 00009: loss improved from 5.25666 to 5.17112, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 653us/sample - loss: 5.1711 - acc: 0.1490\n",
      "Epoch 10/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 5.0810 - acc: 0.1565\n",
      "Epoch 00010: loss improved from 5.17112 to 5.08115, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 633us/sample - loss: 5.0812 - acc: 0.1565\n",
      "Epoch 11/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 4.9864 - acc: 0.1659\n",
      "Epoch 00011: loss improved from 5.08115 to 4.98681, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 661us/sample - loss: 4.9868 - acc: 0.1658\n",
      "Epoch 12/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 4.8991 - acc: 0.1744\n",
      "Epoch 00012: loss improved from 4.98681 to 4.90022, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 659us/sample - loss: 4.9002 - acc: 0.1744\n",
      "Epoch 13/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 4.8212 - acc: 0.1788\n",
      "Epoch 00013: loss improved from 4.90022 to 4.82194, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 688us/sample - loss: 4.8219 - acc: 0.1786\n",
      "Epoch 14/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 4.7486 - acc: 0.1846\n",
      "Epoch 00014: loss improved from 4.82194 to 4.74727, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 686us/sample - loss: 4.7473 - acc: 0.1846\n",
      "Epoch 15/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 4.6730 - acc: 0.1911\n",
      "Epoch 00015: loss improved from 4.74727 to 4.67213, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 745us/sample - loss: 4.6721 - acc: 0.1914\n",
      "Epoch 16/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 4.5939 - acc: 0.1951\n",
      "Epoch 00016: loss improved from 4.67213 to 4.59344, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 691us/sample - loss: 4.5934 - acc: 0.1951\n",
      "Epoch 17/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 4.5106 - acc: 0.2040\n",
      "Epoch 00017: loss improved from 4.59344 to 4.51044, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 677us/sample - loss: 4.5104 - acc: 0.2040\n",
      "Epoch 18/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 4.4275 - acc: 0.2070\n",
      "Epoch 00018: loss improved from 4.51044 to 4.42754, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 685us/sample - loss: 4.4275 - acc: 0.2070\n",
      "Epoch 19/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 4.3400 - acc: 0.2144\n",
      "Epoch 00019: loss improved from 4.42754 to 4.34041, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 740us/sample - loss: 4.3404 - acc: 0.2144\n",
      "Epoch 20/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 4.4076 - acc: 0.2079\n",
      "Epoch 00020: loss did not improve from 4.34041\n",
      "21809/21809 [==============================] - 15s 667us/sample - loss: 4.4073 - acc: 0.2078\n",
      "Epoch 21/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 4.1989 - acc: 0.2207\n",
      "Epoch 00021: loss improved from 4.34041 to 4.19966, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 661us/sample - loss: 4.1997 - acc: 0.2204\n",
      "Epoch 22/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 4.1137 - acc: 0.2273\n",
      "Epoch 00022: loss improved from 4.19966 to 4.11463, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 660us/sample - loss: 4.1146 - acc: 0.2273\n",
      "Epoch 23/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 4.0449 - acc: 0.2329\n",
      "Epoch 00023: loss improved from 4.11463 to 4.04484, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 661us/sample - loss: 4.0448 - acc: 0.2329\n",
      "Epoch 24/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.9715 - acc: 0.2386\n",
      "Epoch 00024: loss improved from 4.04484 to 3.97199, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 666us/sample - loss: 3.9720 - acc: 0.2386\n",
      "Epoch 25/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.9077 - acc: 0.2419\n",
      "Epoch 00025: loss improved from 3.97199 to 3.90719, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 663us/sample - loss: 3.9072 - acc: 0.2418\n",
      "Epoch 26/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.8419 - acc: 0.2470\n",
      "Epoch 00026: loss improved from 3.90719 to 3.84211, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 662us/sample - loss: 3.8421 - acc: 0.2471\n",
      "Epoch 27/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.7781 - acc: 0.2491\n",
      "Epoch 00027: loss improved from 3.84211 to 3.77909, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 660us/sample - loss: 3.7791 - acc: 0.2488\n",
      "Epoch 28/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.7229 - acc: 0.2538\n",
      "Epoch 00028: loss improved from 3.77909 to 3.72298, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 660us/sample - loss: 3.7230 - acc: 0.2539\n",
      "Epoch 29/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.6660 - acc: 0.2596\n",
      "Epoch 00029: loss improved from 3.72298 to 3.66631, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 662us/sample - loss: 3.6663 - acc: 0.2595\n",
      "Epoch 30/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.6102 - acc: 0.2647\n",
      "Epoch 00030: loss improved from 3.66631 to 3.61072, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 669us/sample - loss: 3.6107 - acc: 0.2645\n",
      "Epoch 31/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.5551 - acc: 0.2682\n",
      "Epoch 00031: loss improved from 3.61072 to 3.55602, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 719us/sample - loss: 3.5560 - acc: 0.2683\n",
      "Epoch 32/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.5014 - acc: 0.2750\n",
      "Epoch 00032: loss improved from 3.55602 to 3.50096, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 689us/sample - loss: 3.5010 - acc: 0.2750\n",
      "Epoch 33/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.4588 - acc: 0.2769\n",
      "Epoch 00033: loss improved from 3.50096 to 3.45840, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 701us/sample - loss: 3.4584 - acc: 0.2770\n",
      "Epoch 34/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.4031 - acc: 0.2844\n",
      "Epoch 00034: loss improved from 3.45840 to 3.40244, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 661us/sample - loss: 3.4024 - acc: 0.2845\n",
      "Epoch 35/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.3479 - acc: 0.2902\n",
      "Epoch 00035: loss improved from 3.40244 to 3.34743, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 664us/sample - loss: 3.3474 - acc: 0.2903\n",
      "Epoch 36/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.2977 - acc: 0.2969\n",
      "Epoch 00036: loss improved from 3.34743 to 3.29957, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 657us/sample - loss: 3.2996 - acc: 0.2966\n",
      "Epoch 37/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.2501 - acc: 0.3027\n",
      "Epoch 00037: loss improved from 3.29957 to 3.25019, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 670us/sample - loss: 3.2502 - acc: 0.3027\n",
      "Epoch 38/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.2012 - acc: 0.3123\n",
      "Epoch 00038: loss improved from 3.25019 to 3.20196, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 673us/sample - loss: 3.2020 - acc: 0.3122\n",
      "Epoch 39/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.1533 - acc: 0.3160\n",
      "Epoch 00039: loss improved from 3.20196 to 3.15283, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 667us/sample - loss: 3.1528 - acc: 0.3160\n",
      "Epoch 40/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.1075 - acc: 0.3204\n",
      "Epoch 00040: loss improved from 3.15283 to 3.10767, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 737us/sample - loss: 3.1077 - acc: 0.3204\n",
      "Epoch 41/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.0672 - acc: 0.3255\n",
      "Epoch 00041: loss improved from 3.10767 to 3.06733, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 710us/sample - loss: 3.0673 - acc: 0.3253\n",
      "Epoch 42/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 3.0190 - acc: 0.3308\n",
      "Epoch 00042: loss improved from 3.06733 to 3.01873, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 666us/sample - loss: 3.0187 - acc: 0.3308\n",
      "Epoch 43/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.9775 - acc: 0.3396\n",
      "Epoch 00043: loss improved from 3.01873 to 2.97644, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 657us/sample - loss: 2.9764 - acc: 0.3396\n",
      "Epoch 44/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.9264 - acc: 0.3472\n",
      "Epoch 00044: loss improved from 2.97644 to 2.92762, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 656us/sample - loss: 2.9276 - acc: 0.3472\n",
      "Epoch 45/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.8863 - acc: 0.3538\n",
      "Epoch 00045: loss improved from 2.92762 to 2.88699, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 658us/sample - loss: 2.8870 - acc: 0.3537\n",
      "Epoch 46/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.8390 - acc: 0.3611\n",
      "Epoch 00046: loss improved from 2.88699 to 2.83975, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 660us/sample - loss: 2.8397 - acc: 0.3611\n",
      "Epoch 47/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.7966 - acc: 0.3660\n",
      "Epoch 00047: loss improved from 2.83975 to 2.79802, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 670us/sample - loss: 2.7980 - acc: 0.3658\n",
      "Epoch 48/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.7616 - acc: 0.3708\n",
      "Epoch 00048: loss improved from 2.79802 to 2.76133, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 663us/sample - loss: 2.7613 - acc: 0.3710\n",
      "Epoch 49/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.7250 - acc: 0.3757\n",
      "Epoch 00049: loss improved from 2.76133 to 2.72480, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 666us/sample - loss: 2.7248 - acc: 0.3759\n",
      "Epoch 50/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.6883 - acc: 0.3858\n",
      "Epoch 00050: loss improved from 2.72480 to 2.68809, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 669us/sample - loss: 2.6881 - acc: 0.3858\n",
      "Epoch 51/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.6492 - acc: 0.3901\n",
      "Epoch 00051: loss improved from 2.68809 to 2.64929, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 661us/sample - loss: 2.6493 - acc: 0.3900\n",
      "Epoch 52/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.6108 - acc: 0.3978\n",
      "Epoch 00052: loss improved from 2.64929 to 2.61188, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 664us/sample - loss: 2.6119 - acc: 0.3979\n",
      "Epoch 53/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.5753 - acc: 0.4026\n",
      "Epoch 00053: loss improved from 2.61188 to 2.57490, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 669us/sample - loss: 2.5749 - acc: 0.4025\n",
      "Epoch 54/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.5389 - acc: 0.4097\n",
      "Epoch 00054: loss improved from 2.57490 to 2.53872, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 663us/sample - loss: 2.5387 - acc: 0.4096\n",
      "Epoch 55/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.5037 - acc: 0.4172\n",
      "Epoch 00055: loss improved from 2.53872 to 2.50356, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 667us/sample - loss: 2.5036 - acc: 0.4171\n",
      "Epoch 56/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.4686 - acc: 0.4257\n",
      "Epoch 00056: loss improved from 2.50356 to 2.46895, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 670us/sample - loss: 2.4690 - acc: 0.4259\n",
      "Epoch 57/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.4283 - acc: 0.4304\n",
      "Epoch 00057: loss improved from 2.46895 to 2.42857, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 676us/sample - loss: 2.4286 - acc: 0.4305\n",
      "Epoch 58/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.4016 - acc: 0.4364\n",
      "Epoch 00058: loss improved from 2.42857 to 2.40263, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 665us/sample - loss: 2.4026 - acc: 0.4364\n",
      "Epoch 59/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.3770 - acc: 0.4368\n",
      "Epoch 00059: loss improved from 2.40263 to 2.37642, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 664us/sample - loss: 2.3764 - acc: 0.4369\n",
      "Epoch 60/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.3369 - acc: 0.4444\n",
      "Epoch 00060: loss improved from 2.37642 to 2.33542, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 668us/sample - loss: 2.3354 - acc: 0.4448\n",
      "Epoch 61/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.3005 - acc: 0.4523\n",
      "Epoch 00061: loss improved from 2.33542 to 2.30122, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 665us/sample - loss: 2.3012 - acc: 0.4523\n",
      "Epoch 62/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.2792 - acc: 0.4565\n",
      "Epoch 00062: loss improved from 2.30122 to 2.27912, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 669us/sample - loss: 2.2791 - acc: 0.4565\n",
      "Epoch 63/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.2376 - acc: 0.4676\n",
      "Epoch 00063: loss improved from 2.27912 to 2.23783, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 667us/sample - loss: 2.2378 - acc: 0.4676\n",
      "Epoch 64/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.2065 - acc: 0.4726\n",
      "Epoch 00064: loss improved from 2.23783 to 2.20727, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 669us/sample - loss: 2.2073 - acc: 0.4724\n",
      "Epoch 65/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.1851 - acc: 0.4765\n",
      "Epoch 00065: loss improved from 2.20727 to 2.18450, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 671us/sample - loss: 2.1845 - acc: 0.4766\n",
      "Epoch 66/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.1493 - acc: 0.4819\n",
      "Epoch 00066: loss improved from 2.18450 to 2.14975, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 669us/sample - loss: 2.1497 - acc: 0.4819\n",
      "Epoch 67/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.1227 - acc: 0.4882\n",
      "Epoch 00067: loss improved from 2.14975 to 2.12221, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 680us/sample - loss: 2.1222 - acc: 0.4882\n",
      "Epoch 68/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.0900 - acc: 0.4961\n",
      "Epoch 00068: loss improved from 2.12221 to 2.09005, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 684us/sample - loss: 2.0900 - acc: 0.4961\n",
      "Epoch 69/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.0573 - acc: 0.5034\n",
      "Epoch 00069: loss improved from 2.09005 to 2.05795, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 668us/sample - loss: 2.0580 - acc: 0.5031\n",
      "Epoch 70/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.0301 - acc: 0.5082\n",
      "Epoch 00070: loss improved from 2.05795 to 2.03035, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 664us/sample - loss: 2.0303 - acc: 0.5084\n",
      "Epoch 71/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 2.0133 - acc: 0.5097\n",
      "Epoch 00071: loss improved from 2.03035 to 2.01387, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 662us/sample - loss: 2.0139 - acc: 0.5097\n",
      "Epoch 72/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.9804 - acc: 0.5171\n",
      "Epoch 00072: loss improved from 2.01387 to 1.98014, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 660us/sample - loss: 1.9801 - acc: 0.5171\n",
      "Epoch 73/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.9564 - acc: 0.5206\n",
      "Epoch 00073: loss improved from 1.98014 to 1.95587, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 708us/sample - loss: 1.9559 - acc: 0.5210\n",
      "Epoch 74/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.9153 - acc: 0.5318\n",
      "Epoch 00074: loss improved from 1.95587 to 1.91587, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 701us/sample - loss: 1.9159 - acc: 0.5318\n",
      "Epoch 75/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.8883 - acc: 0.5371\n",
      "Epoch 00075: loss improved from 1.91587 to 1.88846, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 666us/sample - loss: 1.8885 - acc: 0.5371\n",
      "Epoch 76/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.8677 - acc: 0.5396\n",
      "Epoch 00076: loss improved from 1.88846 to 1.86828, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 662us/sample - loss: 1.8683 - acc: 0.5395\n",
      "Epoch 77/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.8379 - acc: 0.5455\n",
      "Epoch 00077: loss improved from 1.86828 to 1.83825, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 663us/sample - loss: 1.8382 - acc: 0.5456\n",
      "Epoch 78/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.8143 - acc: 0.5490\n",
      "Epoch 00078: loss improved from 1.83825 to 1.81475, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 662us/sample - loss: 1.8147 - acc: 0.5489\n",
      "Epoch 79/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.7975 - acc: 0.5566\n",
      "Epoch 00079: loss improved from 1.81475 to 1.79701, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 661us/sample - loss: 1.7970 - acc: 0.5567\n",
      "Epoch 80/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.7553 - acc: 0.5656\n",
      "Epoch 00080: loss improved from 1.79701 to 1.75499, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 722us/sample - loss: 1.7550 - acc: 0.5659\n",
      "Epoch 81/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.7453 - acc: 0.5627\n",
      "Epoch 00081: loss improved from 1.75499 to 1.74465, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 696us/sample - loss: 1.7446 - acc: 0.5628\n",
      "Epoch 82/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.7166 - acc: 0.5756\n",
      "Epoch 00082: loss improved from 1.74465 to 1.71678, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 698us/sample - loss: 1.7168 - acc: 0.5754\n",
      "Epoch 83/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.6884 - acc: 0.5802\n",
      "Epoch 00083: loss improved from 1.71678 to 1.68821, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 17s 758us/sample - loss: 1.6882 - acc: 0.5802\n",
      "Epoch 84/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.6708 - acc: 0.5822\n",
      "Epoch 00084: loss improved from 1.68821 to 1.67129, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 686us/sample - loss: 1.6713 - acc: 0.5820\n",
      "Epoch 85/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.6399 - acc: 0.5915\n",
      "Epoch 00085: loss improved from 1.67129 to 1.63987, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 685us/sample - loss: 1.6399 - acc: 0.5915\n",
      "Epoch 86/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.6181 - acc: 0.5938\n",
      "Epoch 00086: loss improved from 1.63987 to 1.61884, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 709us/sample - loss: 1.6188 - acc: 0.5935\n",
      "Epoch 87/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.5916 - acc: 0.6010\n",
      "Epoch 00087: loss improved from 1.61884 to 1.59193, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 693us/sample - loss: 1.5919 - acc: 0.6007\n",
      "Epoch 88/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.5742 - acc: 0.6039\n",
      "Epoch 00088: loss improved from 1.59193 to 1.57441, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 684us/sample - loss: 1.5744 - acc: 0.6039\n",
      "Epoch 89/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.5474 - acc: 0.6112\n",
      "Epoch 00089: loss improved from 1.57441 to 1.54672, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 666us/sample - loss: 1.5467 - acc: 0.6114\n",
      "Epoch 90/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.5323 - acc: 0.6128\n",
      "Epoch 00090: loss improved from 1.54672 to 1.53194, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 655us/sample - loss: 1.5319 - acc: 0.6128\n",
      "Epoch 91/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.5015 - acc: 0.6196\n",
      "Epoch 00091: loss improved from 1.53194 to 1.50146, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 655us/sample - loss: 1.5015 - acc: 0.6196\n",
      "Epoch 92/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.4784 - acc: 0.6237\n",
      "Epoch 00092: loss improved from 1.50146 to 1.47891, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 664us/sample - loss: 1.4789 - acc: 0.6235\n",
      "Epoch 93/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.4617 - acc: 0.6294\n",
      "Epoch 00093: loss improved from 1.47891 to 1.46163, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 661us/sample - loss: 1.4616 - acc: 0.6293\n",
      "Epoch 94/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.4277 - acc: 0.6385\n",
      "Epoch 00094: loss improved from 1.46163 to 1.42801, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 660us/sample - loss: 1.4280 - acc: 0.6385\n",
      "Epoch 95/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.4115 - acc: 0.6417\n",
      "Epoch 00095: loss improved from 1.42801 to 1.41181, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 666us/sample - loss: 1.4118 - acc: 0.6414\n",
      "Epoch 96/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.3919 - acc: 0.6459\n",
      "Epoch 00096: loss improved from 1.41181 to 1.39259, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 670us/sample - loss: 1.3926 - acc: 0.6456\n",
      "Epoch 97/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.3719 - acc: 0.6523\n",
      "Epoch 00097: loss improved from 1.39259 to 1.37202, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 671us/sample - loss: 1.3720 - acc: 0.6524\n",
      "Epoch 98/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.3512 - acc: 0.6574\n",
      "Epoch 00098: loss improved from 1.37202 to 1.35212, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 669us/sample - loss: 1.3521 - acc: 0.6571\n",
      "Epoch 99/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.3279 - acc: 0.6627\n",
      "Epoch 00099: loss improved from 1.35212 to 1.32763, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 671us/sample - loss: 1.3276 - acc: 0.6628\n",
      "Epoch 100/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.2982 - acc: 0.6704\n",
      "Epoch 00100: loss improved from 1.32763 to 1.29901, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 671us/sample - loss: 1.2990 - acc: 0.6701\n",
      "Epoch 101/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.2892 - acc: 0.6710\n",
      "Epoch 00101: loss improved from 1.29901 to 1.28926, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 669us/sample - loss: 1.2893 - acc: 0.6711\n",
      "Epoch 102/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.2786 - acc: 0.6698\n",
      "Epoch 00102: loss improved from 1.28926 to 1.27911, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 669us/sample - loss: 1.2791 - acc: 0.6698\n",
      "Epoch 103/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.2428 - acc: 0.6818\n",
      "Epoch 00103: loss improved from 1.27911 to 1.24309, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 676us/sample - loss: 1.2431 - acc: 0.6816\n",
      "Epoch 104/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.2197 - acc: 0.6905\n",
      "Epoch 00104: loss improved from 1.24309 to 1.21946, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 667us/sample - loss: 1.2195 - acc: 0.6908\n",
      "Epoch 105/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.2131 - acc: 0.6886\n",
      "Epoch 00105: loss improved from 1.21946 to 1.21309, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 661us/sample - loss: 1.2131 - acc: 0.6886\n",
      "Epoch 106/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.1898 - acc: 0.6943\n",
      "Epoch 00106: loss improved from 1.21309 to 1.19005, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 676us/sample - loss: 1.1900 - acc: 0.6940\n",
      "Epoch 107/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.1635 - acc: 0.7051\n",
      "Epoch 00107: loss improved from 1.19005 to 1.16350, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 665us/sample - loss: 1.1635 - acc: 0.7051\n",
      "Epoch 108/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.1550 - acc: 0.7036\n",
      "Epoch 00108: loss improved from 1.16350 to 1.15568, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 677us/sample - loss: 1.1557 - acc: 0.7033\n",
      "Epoch 109/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.1323 - acc: 0.7096\n",
      "Epoch 00109: loss improved from 1.15568 to 1.13183, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 680us/sample - loss: 1.1318 - acc: 0.7097\n",
      "Epoch 110/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.1032 - acc: 0.7189\n",
      "Epoch 00110: loss improved from 1.13183 to 1.10359, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 687us/sample - loss: 1.1036 - acc: 0.7186\n",
      "Epoch 111/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.0982 - acc: 0.7164\n",
      "Epoch 00111: loss improved from 1.10359 to 1.09833, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 687us/sample - loss: 1.0983 - acc: 0.7163\n",
      "Epoch 112/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.0763 - acc: 0.7232\n",
      "Epoch 00112: loss improved from 1.09833 to 1.07707, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 683us/sample - loss: 1.0771 - acc: 0.7231\n",
      "Epoch 113/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.0543 - acc: 0.7294\n",
      "Epoch 00113: loss improved from 1.07707 to 1.05408, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 679us/sample - loss: 1.0541 - acc: 0.7293\n",
      "Epoch 114/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.0365 - acc: 0.7357\n",
      "Epoch 00114: loss improved from 1.05408 to 1.03677, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 684us/sample - loss: 1.0368 - acc: 0.7355\n",
      "Epoch 115/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.0144 - acc: 0.7409\n",
      "Epoch 00115: loss improved from 1.03677 to 1.01427, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 676us/sample - loss: 1.0143 - acc: 0.7411\n",
      "Epoch 116/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.0037 - acc: 0.7435\n",
      "Epoch 00116: loss improved from 1.01427 to 1.00432, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 675us/sample - loss: 1.0043 - acc: 0.7434\n",
      "Epoch 117/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.9995 - acc: 0.7433\n",
      "Epoch 00117: loss improved from 1.00432 to 0.99945, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 677us/sample - loss: 0.9994 - acc: 0.7431\n",
      "Epoch 118/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.9649 - acc: 0.7532\n",
      "Epoch 00118: loss improved from 0.99945 to 0.96455, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 677us/sample - loss: 0.9646 - acc: 0.7532\n",
      "Epoch 119/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.9461 - acc: 0.7563\n",
      "Epoch 00119: loss improved from 0.96455 to 0.94632, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 676us/sample - loss: 0.9463 - acc: 0.7562\n",
      "Epoch 120/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.9391 - acc: 0.7595\n",
      "Epoch 00120: loss improved from 0.94632 to 0.93897, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 687us/sample - loss: 0.9390 - acc: 0.7595\n",
      "Epoch 121/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.9125 - acc: 0.7660\n",
      "Epoch 00121: loss improved from 0.93897 to 0.91302, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 678us/sample - loss: 0.9130 - acc: 0.7659\n",
      "Epoch 122/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.9017 - acc: 0.7698\n",
      "Epoch 00122: loss improved from 0.91302 to 0.90210, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 677us/sample - loss: 0.9021 - acc: 0.7697\n",
      "Epoch 123/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.8813 - acc: 0.7741\n",
      "Epoch 00123: loss improved from 0.90210 to 0.88117, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 684us/sample - loss: 0.8812 - acc: 0.7742\n",
      "Epoch 124/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.8476 - acc: 0.7860\n",
      "Epoch 00124: loss improved from 0.88117 to 0.84764, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 695us/sample - loss: 0.8476 - acc: 0.7859\n",
      "Epoch 125/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.8434 - acc: 0.7868\n",
      "Epoch 00125: loss improved from 0.84764 to 0.84326, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 689us/sample - loss: 0.8433 - acc: 0.7869\n",
      "Epoch 126/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.8343 - acc: 0.7861\n",
      "Epoch 00126: loss improved from 0.84326 to 0.83517, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 693us/sample - loss: 0.8352 - acc: 0.7859\n",
      "Epoch 127/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.8427 - acc: 0.7831\n",
      "Epoch 00127: loss did not improve from 0.83517\n",
      "21809/21809 [==============================] - 14s 661us/sample - loss: 0.8427 - acc: 0.7832\n",
      "Epoch 128/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.7985 - acc: 0.7965\n",
      "Epoch 00128: loss improved from 0.83517 to 0.79773, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 668us/sample - loss: 0.7977 - acc: 0.7968\n",
      "Epoch 129/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.7722 - acc: 0.8052\n",
      "Epoch 00129: loss improved from 0.79773 to 0.77167, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 668us/sample - loss: 0.7717 - acc: 0.8054\n",
      "Epoch 130/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.7669 - acc: 0.8051\n",
      "Epoch 00130: loss improved from 0.77167 to 0.76743, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 667us/sample - loss: 0.7674 - acc: 0.8050\n",
      "Epoch 131/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.7674 - acc: 0.8029\n",
      "Epoch 00131: loss did not improve from 0.76743\n",
      "21809/21809 [==============================] - 14s 662us/sample - loss: 0.7677 - acc: 0.8027\n",
      "Epoch 132/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.7406 - acc: 0.8132\n",
      "Epoch 00132: loss improved from 0.76743 to 0.74128, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 671us/sample - loss: 0.7413 - acc: 0.8128\n",
      "Epoch 133/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.7168 - acc: 0.8209\n",
      "Epoch 00133: loss improved from 0.74128 to 0.71701, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 749us/sample - loss: 0.7170 - acc: 0.8208\n",
      "Epoch 134/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.7011 - acc: 0.8235\n",
      "Epoch 00134: loss improved from 0.71701 to 0.70114, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 719us/sample - loss: 0.7011 - acc: 0.8236\n",
      "Epoch 135/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.7044 - acc: 0.8218\n",
      "Epoch 00135: loss did not improve from 0.70114\n",
      "21809/21809 [==============================] - 15s 676us/sample - loss: 0.7040 - acc: 0.8219\n",
      "Epoch 136/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.8291\n",
      "Epoch 00136: loss improved from 0.70114 to 0.68133, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 664us/sample - loss: 0.6813 - acc: 0.8291\n",
      "Epoch 137/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.6629 - acc: 0.8334\n",
      "Epoch 00137: loss improved from 0.68133 to 0.66277, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 666us/sample - loss: 0.6628 - acc: 0.8336\n",
      "Epoch 138/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.6366 - acc: 0.8438\n",
      "Epoch 00138: loss improved from 0.66277 to 0.63648, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 663us/sample - loss: 0.6365 - acc: 0.8439\n",
      "Epoch 139/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.6105 - acc: 0.8493\n",
      "Epoch 00139: loss improved from 0.63648 to 0.61068, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 665us/sample - loss: 0.6107 - acc: 0.8492\n",
      "Epoch 140/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.6061 - acc: 0.8501\n",
      "Epoch 00140: loss improved from 0.61068 to 0.60690, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 668us/sample - loss: 0.6069 - acc: 0.8499\n",
      "Epoch 141/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.6159 - acc: 0.8456\n",
      "Epoch 00141: loss did not improve from 0.60690\n",
      "21809/21809 [==============================] - 15s 666us/sample - loss: 0.6154 - acc: 0.8457\n",
      "Epoch 142/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.6090 - acc: 0.8448\n",
      "Epoch 00142: loss did not improve from 0.60690\n",
      "21809/21809 [==============================] - 15s 673us/sample - loss: 0.6092 - acc: 0.8447\n",
      "Epoch 143/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.5919 - acc: 0.8503\n",
      "Epoch 00143: loss improved from 0.60690 to 0.59219, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 663us/sample - loss: 0.5922 - acc: 0.8502\n",
      "Epoch 144/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.5672 - acc: 0.8606\n",
      "Epoch 00144: loss improved from 0.59219 to 0.56744, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 668us/sample - loss: 0.5674 - acc: 0.8605\n",
      "Epoch 145/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.8624\n",
      "Epoch 00145: loss improved from 0.56744 to 0.55071, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 664us/sample - loss: 0.5507 - acc: 0.8624\n",
      "Epoch 146/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.8689\n",
      "Epoch 00146: loss improved from 0.55071 to 0.53858, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 662us/sample - loss: 0.5386 - acc: 0.8688\n",
      "Epoch 147/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.5509 - acc: 0.8638\n",
      "Epoch 00147: loss did not improve from 0.53858\n",
      "21809/21809 [==============================] - 14s 661us/sample - loss: 0.5507 - acc: 0.8640\n",
      "Epoch 148/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.8690\n",
      "Epoch 00148: loss improved from 0.53858 to 0.52955, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 663us/sample - loss: 0.5295 - acc: 0.8690\n",
      "Epoch 149/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.8753\n",
      "Epoch 00149: loss improved from 0.52955 to 0.50811, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 661us/sample - loss: 0.5081 - acc: 0.8754\n",
      "Epoch 150/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4856 - acc: 0.8813\n",
      "Epoch 00150: loss improved from 0.50811 to 0.48556, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 665us/sample - loss: 0.4856 - acc: 0.8814\n",
      "Epoch 151/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4555 - acc: 0.8924\n",
      "Epoch 00151: loss improved from 0.48556 to 0.45556, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 665us/sample - loss: 0.4556 - acc: 0.8924\n",
      "Epoch 152/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4417 - acc: 0.8972\n",
      "Epoch 00152: loss improved from 0.45556 to 0.44208, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 665us/sample - loss: 0.4421 - acc: 0.8971\n",
      "Epoch 153/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4375 - acc: 0.8983\n",
      "Epoch 00153: loss improved from 0.44208 to 0.43739, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 666us/sample - loss: 0.4374 - acc: 0.8983\n",
      "Epoch 154/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4356 - acc: 0.8971\n",
      "Epoch 00154: loss improved from 0.43739 to 0.43581, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 662us/sample - loss: 0.4358 - acc: 0.8970\n",
      "Epoch 155/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4410 - acc: 0.8927\n",
      "Epoch 00155: loss did not improve from 0.43581\n",
      "21809/21809 [==============================] - 15s 679us/sample - loss: 0.4412 - acc: 0.8928\n",
      "Epoch 156/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4461 - acc: 0.8893\n",
      "Epoch 00156: loss did not improve from 0.43581\n",
      "21809/21809 [==============================] - 16s 725us/sample - loss: 0.4464 - acc: 0.8891\n",
      "Epoch 157/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8925\n",
      "Epoch 00157: loss improved from 0.43581 to 0.43503, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 703us/sample - loss: 0.4350 - acc: 0.8924\n",
      "Epoch 158/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4389 - acc: 0.8916\n",
      "Epoch 00158: loss did not improve from 0.43503\n",
      "21809/21809 [==============================] - 16s 726us/sample - loss: 0.4387 - acc: 0.8916\n",
      "Epoch 159/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4290 - acc: 0.8968\n",
      "Epoch 00159: loss improved from 0.43503 to 0.42863, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 729us/sample - loss: 0.4286 - acc: 0.8969\n",
      "Epoch 160/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3808 - acc: 0.9109\n",
      "Epoch 00160: loss improved from 0.42863 to 0.38081, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 707us/sample - loss: 0.3808 - acc: 0.9108\n",
      "Epoch 161/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.9194\n",
      "Epoch 00161: loss improved from 0.38081 to 0.35528, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 735us/sample - loss: 0.3553 - acc: 0.9193\n",
      "Epoch 162/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3421 - acc: 0.9247\n",
      "Epoch 00162: loss improved from 0.35528 to 0.34188, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 710us/sample - loss: 0.3419 - acc: 0.9248\n",
      "Epoch 163/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.9279\n",
      "Epoch 00163: loss improved from 0.34188 to 0.32469, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 709us/sample - loss: 0.3247 - acc: 0.9279\n",
      "Epoch 164/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.9343\n",
      "Epoch 00164: loss improved from 0.32469 to 0.31371, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 704us/sample - loss: 0.3137 - acc: 0.9342\n",
      "Epoch 165/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.9290\n",
      "Epoch 00165: loss did not improve from 0.31371\n",
      "21809/21809 [==============================] - 16s 753us/sample - loss: 0.3179 - acc: 0.9291\n",
      "Epoch 166/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.9334\n",
      "Epoch 00166: loss did not improve from 0.31371\n",
      "21809/21809 [==============================] - 16s 713us/sample - loss: 0.3137 - acc: 0.9334\n",
      "Epoch 167/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.9295\n",
      "Epoch 00167: loss did not improve from 0.31371\n",
      "21809/21809 [==============================] - 16s 742us/sample - loss: 0.3160 - acc: 0.9295\n",
      "Epoch 168/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3694 - acc: 0.9082\n",
      "Epoch 00168: loss did not improve from 0.31371\n",
      "21809/21809 [==============================] - 17s 791us/sample - loss: 0.3698 - acc: 0.9081\n",
      "Epoch 169/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4059 - acc: 0.8942\n",
      "Epoch 00169: loss did not improve from 0.31371\n",
      "21809/21809 [==============================] - 15s 706us/sample - loss: 0.4059 - acc: 0.8941\n",
      "Epoch 170/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3594 - acc: 0.9124\n",
      "Epoch 00170: loss did not improve from 0.31371\n",
      "21809/21809 [==============================] - 15s 709us/sample - loss: 0.3593 - acc: 0.9124\n",
      "Epoch 171/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.9314\n",
      "Epoch 00171: loss improved from 0.31371 to 0.30375, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 707us/sample - loss: 0.3037 - acc: 0.9314\n",
      "Epoch 172/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9477\n",
      "Epoch 00172: loss improved from 0.30375 to 0.25848, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 705us/sample - loss: 0.2585 - acc: 0.9477\n",
      "Epoch 173/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9584\n",
      "Epoch 00173: loss improved from 0.25848 to 0.22409, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 707us/sample - loss: 0.2241 - acc: 0.9584\n",
      "Epoch 174/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9646\n",
      "Epoch 00174: loss improved from 0.22409 to 0.20239, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 715us/sample - loss: 0.2024 - acc: 0.9646\n",
      "Epoch 175/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9675\n",
      "Epoch 00175: loss improved from 0.20239 to 0.19380, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 709us/sample - loss: 0.1938 - acc: 0.9674\n",
      "Epoch 176/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9664\n",
      "Epoch 00176: loss did not improve from 0.19380\n",
      "21809/21809 [==============================] - 16s 753us/sample - loss: 0.1948 - acc: 0.9664\n",
      "Epoch 177/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9534\n",
      "Epoch 00177: loss did not improve from 0.19380\n",
      "21809/21809 [==============================] - 15s 710us/sample - loss: 0.2280 - acc: 0.9532\n",
      "Epoch 178/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3100 - acc: 0.9218\n",
      "Epoch 00178: loss did not improve from 0.19380\n",
      "21809/21809 [==============================] - 15s 709us/sample - loss: 0.3098 - acc: 0.9219\n",
      "Epoch 179/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8820\n",
      "Epoch 00179: loss did not improve from 0.19380\n",
      "21809/21809 [==============================] - 16s 712us/sample - loss: 0.4191 - acc: 0.8819\n",
      "Epoch 180/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.9037\n",
      "Epoch 00180: loss did not improve from 0.19380\n",
      "21809/21809 [==============================] - 16s 719us/sample - loss: 0.3577 - acc: 0.9038\n",
      "Epoch 181/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9426\n",
      "Epoch 00181: loss did not improve from 0.19380\n",
      "21809/21809 [==============================] - 16s 715us/sample - loss: 0.2589 - acc: 0.9427\n",
      "Epoch 182/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9629\n",
      "Epoch 00182: loss did not improve from 0.19380\n",
      "21809/21809 [==============================] - 16s 714us/sample - loss: 0.1949 - acc: 0.9628\n",
      "Epoch 183/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9738\n",
      "Epoch 00183: loss improved from 0.19380 to 0.16455, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 716us/sample - loss: 0.1646 - acc: 0.9738\n",
      "Epoch 184/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9790\n",
      "Epoch 00184: loss improved from 0.16455 to 0.14158, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 712us/sample - loss: 0.1416 - acc: 0.9789\n",
      "Epoch 185/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9825\n",
      "Epoch 00185: loss improved from 0.14158 to 0.12911, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 717us/sample - loss: 0.1291 - acc: 0.9824\n",
      "Epoch 186/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9844\n",
      "Epoch 00186: loss improved from 0.12911 to 0.12404, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 727us/sample - loss: 0.1240 - acc: 0.9845\n",
      "Epoch 187/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9860\n",
      "Epoch 00187: loss improved from 0.12404 to 0.11859, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 718us/sample - loss: 0.1186 - acc: 0.9859\n",
      "Epoch 188/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9841\n",
      "Epoch 00188: loss did not improve from 0.11859\n",
      "21809/21809 [==============================] - 16s 713us/sample - loss: 0.1237 - acc: 0.9841\n",
      "Epoch 189/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9735\n",
      "Epoch 00189: loss did not improve from 0.11859\n",
      "21809/21809 [==============================] - 16s 715us/sample - loss: 0.1552 - acc: 0.9734\n",
      "Epoch 190/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.9174\n",
      "Epoch 00190: loss did not improve from 0.11859\n",
      "21809/21809 [==============================] - 16s 721us/sample - loss: 0.3102 - acc: 0.9173\n",
      "Epoch 191/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.8462\n",
      "Epoch 00191: loss did not improve from 0.11859\n",
      "21809/21809 [==============================] - 16s 716us/sample - loss: 0.5302 - acc: 0.8462\n",
      "Epoch 192/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3397 - acc: 0.9060\n",
      "Epoch 00192: loss did not improve from 0.11859\n",
      "21809/21809 [==============================] - 16s 716us/sample - loss: 0.3400 - acc: 0.9059\n",
      "Epoch 193/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9530\n",
      "Epoch 00193: loss did not improve from 0.11859\n",
      "21809/21809 [==============================] - 16s 715us/sample - loss: 0.2106 - acc: 0.9531\n",
      "Epoch 194/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9785\n",
      "Epoch 00194: loss did not improve from 0.11859\n",
      "21809/21809 [==============================] - 16s 714us/sample - loss: 0.1363 - acc: 0.9785\n",
      "Epoch 195/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9886\n",
      "Epoch 00195: loss improved from 0.11859 to 0.09911, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 715us/sample - loss: 0.0991 - acc: 0.9885\n",
      "Epoch 196/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9904\n",
      "Epoch 00196: loss improved from 0.09911 to 0.08726, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 720us/sample - loss: 0.0873 - acc: 0.9904\n",
      "Epoch 197/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9927\n",
      "Epoch 00197: loss improved from 0.08726 to 0.07894, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 717us/sample - loss: 0.0789 - acc: 0.9927\n",
      "Epoch 198/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9935\n",
      "Epoch 00198: loss improved from 0.07894 to 0.07236, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 720us/sample - loss: 0.0724 - acc: 0.9935\n",
      "Epoch 199/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9937\n",
      "Epoch 00199: loss improved from 0.07236 to 0.06897, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 727us/sample - loss: 0.0690 - acc: 0.9937\n",
      "Epoch 200/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9949\n",
      "Epoch 00200: loss improved from 0.06897 to 0.06664, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 728us/sample - loss: 0.0666 - acc: 0.9948\n",
      "Epoch 201/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9937\n",
      "Epoch 00201: loss did not improve from 0.06664\n",
      "21809/21809 [==============================] - 16s 732us/sample - loss: 0.0673 - acc: 0.9938\n",
      "Epoch 202/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9870\n",
      "Epoch 00202: loss did not improve from 0.06664\n",
      "21809/21809 [==============================] - 16s 728us/sample - loss: 0.1004 - acc: 0.9869\n",
      "Epoch 203/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.6310 - acc: 0.8181\n",
      "Epoch 00203: loss did not improve from 0.06664\n",
      "21809/21809 [==============================] - 16s 737us/sample - loss: 0.6310 - acc: 0.8181\n",
      "Epoch 204/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4835 - acc: 0.8566\n",
      "Epoch 00204: loss did not improve from 0.06664\n",
      "21809/21809 [==============================] - 16s 724us/sample - loss: 0.4837 - acc: 0.8565\n",
      "Epoch 205/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9429\n",
      "Epoch 00205: loss did not improve from 0.06664\n",
      "21809/21809 [==============================] - 16s 729us/sample - loss: 0.2267 - acc: 0.9430\n",
      "Epoch 206/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9812\n",
      "Epoch 00206: loss did not improve from 0.06664\n",
      "21809/21809 [==============================] - 16s 727us/sample - loss: 0.1193 - acc: 0.9813\n",
      "Epoch 207/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9929\n",
      "Epoch 00207: loss did not improve from 0.06664\n",
      "21809/21809 [==============================] - 16s 741us/sample - loss: 0.0776 - acc: 0.9929\n",
      "Epoch 208/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9954\n",
      "Epoch 00208: loss improved from 0.06664 to 0.06267, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 726us/sample - loss: 0.0627 - acc: 0.9954\n",
      "Epoch 209/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9966\n",
      "Epoch 00209: loss improved from 0.06267 to 0.05417, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 722us/sample - loss: 0.0542 - acc: 0.9966\n",
      "Epoch 210/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9970\n",
      "Epoch 00210: loss improved from 0.05417 to 0.04947, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 720us/sample - loss: 0.0495 - acc: 0.9970\n",
      "Epoch 211/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9972\n",
      "Epoch 00211: loss improved from 0.04947 to 0.04694, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 730us/sample - loss: 0.0469 - acc: 0.9972\n",
      "Epoch 212/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9976\n",
      "Epoch 00212: loss improved from 0.04694 to 0.04345, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 729us/sample - loss: 0.0434 - acc: 0.9976\n",
      "Epoch 213/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9977\n",
      "Epoch 00213: loss improved from 0.04345 to 0.04191, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 729us/sample - loss: 0.0419 - acc: 0.9977\n",
      "Epoch 214/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9979\n",
      "Epoch 00214: loss improved from 0.04191 to 0.04000, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 725us/sample - loss: 0.0400 - acc: 0.9979\n",
      "Epoch 215/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9982\n",
      "Epoch 00215: loss improved from 0.04000 to 0.03913, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 726us/sample - loss: 0.0391 - acc: 0.9982\n",
      "Epoch 216/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9981\n",
      "Epoch 00216: loss improved from 0.03913 to 0.03750, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 739us/sample - loss: 0.0375 - acc: 0.9981\n",
      "Epoch 217/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9985\n",
      "Epoch 00217: loss improved from 0.03750 to 0.03599, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 734us/sample - loss: 0.0360 - acc: 0.9985\n",
      "Epoch 218/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9983\n",
      "Epoch 00218: loss did not improve from 0.03599\n",
      "21809/21809 [==============================] - 16s 732us/sample - loss: 0.0363 - acc: 0.9983\n",
      "Epoch 219/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9982\n",
      "Epoch 00219: loss did not improve from 0.03599\n",
      "21809/21809 [==============================] - 16s 736us/sample - loss: 0.0384 - acc: 0.9982\n",
      "Epoch 220/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.9087 - acc: 0.7658\n",
      "Epoch 00220: loss did not improve from 0.03599\n",
      "21809/21809 [==============================] - 16s 738us/sample - loss: 0.9089 - acc: 0.7656\n",
      "Epoch 221/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.7065 - acc: 0.8005\n",
      "Epoch 00221: loss did not improve from 0.03599\n",
      "21809/21809 [==============================] - 16s 734us/sample - loss: 0.7057 - acc: 0.8006\n",
      "Epoch 222/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9330\n",
      "Epoch 00222: loss did not improve from 0.03599\n",
      "21809/21809 [==============================] - 16s 742us/sample - loss: 0.2409 - acc: 0.9330\n",
      "Epoch 223/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9855\n",
      "Epoch 00223: loss did not improve from 0.03599\n",
      "21809/21809 [==============================] - 16s 740us/sample - loss: 0.0958 - acc: 0.9855\n",
      "Epoch 224/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9959\n",
      "Epoch 00224: loss did not improve from 0.03599\n",
      "21809/21809 [==============================] - 16s 739us/sample - loss: 0.0557 - acc: 0.9959\n",
      "Epoch 225/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9977\n",
      "Epoch 00225: loss did not improve from 0.03599\n",
      "21809/21809 [==============================] - 16s 741us/sample - loss: 0.0421 - acc: 0.9977\n",
      "Epoch 226/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9985\n",
      "Epoch 00226: loss improved from 0.03599 to 0.03591, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 741us/sample - loss: 0.0359 - acc: 0.9985\n",
      "Epoch 227/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9988\n",
      "Epoch 00227: loss improved from 0.03591 to 0.03215, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 743us/sample - loss: 0.0321 - acc: 0.9988\n",
      "Epoch 228/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9988\n",
      "Epoch 00228: loss improved from 0.03215 to 0.03112, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 742us/sample - loss: 0.0311 - acc: 0.9988\n",
      "Epoch 229/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9987\n",
      "Epoch 00229: loss improved from 0.03112 to 0.02902, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 738us/sample - loss: 0.0290 - acc: 0.9987\n",
      "Epoch 230/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9990\n",
      "Epoch 00230: loss improved from 0.02902 to 0.02752, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 754us/sample - loss: 0.0275 - acc: 0.9990\n",
      "Epoch 231/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9990\n",
      "Epoch 00231: loss improved from 0.02752 to 0.02599, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 696us/sample - loss: 0.0260 - acc: 0.9990\n",
      "Epoch 232/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9991\n",
      "Epoch 00232: loss improved from 0.02599 to 0.02484, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 727us/sample - loss: 0.0248 - acc: 0.9991\n",
      "Epoch 233/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9992\n",
      "Epoch 00233: loss improved from 0.02484 to 0.02452, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 647us/sample - loss: 0.0245 - acc: 0.9992\n",
      "Epoch 234/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9991\n",
      "Epoch 00234: loss improved from 0.02452 to 0.02322, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 646us/sample - loss: 0.0232 - acc: 0.9991\n",
      "Epoch 235/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9991\n",
      "Epoch 00235: loss improved from 0.02322 to 0.02310, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 646us/sample - loss: 0.0231 - acc: 0.9991\n",
      "Epoch 236/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9987\n",
      "Epoch 00236: loss did not improve from 0.02310\n",
      "21809/21809 [==============================] - 14s 646us/sample - loss: 0.0234 - acc: 0.9987\n",
      "Epoch 237/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9989\n",
      "Epoch 00237: loss did not improve from 0.02310\n",
      "21809/21809 [==============================] - 14s 661us/sample - loss: 0.0241 - acc: 0.9989\n",
      "Epoch 238/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9850\n",
      "Epoch 00238: loss did not improve from 0.02310\n",
      "21809/21809 [==============================] - 13s 615us/sample - loss: 0.0735 - acc: 0.9846\n",
      "Epoch 239/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.1172 - acc: 0.7086\n",
      "Epoch 00239: loss did not improve from 0.02310\n",
      "21809/21809 [==============================] - 13s 609us/sample - loss: 1.1167 - acc: 0.7087\n",
      "Epoch 240/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.4384 - acc: 0.8714\n",
      "Epoch 00240: loss did not improve from 0.02310\n",
      "21809/21809 [==============================] - 15s 672us/sample - loss: 0.4385 - acc: 0.8713\n",
      "Epoch 241/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9608\n",
      "Epoch 00241: loss did not improve from 0.02310\n",
      "21809/21809 [==============================] - 14s 638us/sample - loss: 0.1581 - acc: 0.9608\n",
      "Epoch 242/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9916\n",
      "Epoch 00242: loss did not improve from 0.02310\n",
      "21809/21809 [==============================] - 14s 626us/sample - loss: 0.0673 - acc: 0.9916\n",
      "Epoch 243/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9983\n",
      "Epoch 00243: loss did not improve from 0.02310\n",
      "21809/21809 [==============================] - 15s 672us/sample - loss: 0.0370 - acc: 0.9983\n",
      "Epoch 244/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9989\n",
      "Epoch 00244: loss did not improve from 0.02310\n",
      "21809/21809 [==============================] - 14s 628us/sample - loss: 0.0279 - acc: 0.9989\n",
      "Epoch 245/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9989\n",
      "Epoch 00245: loss did not improve from 0.02310\n",
      "21809/21809 [==============================] - 14s 638us/sample - loss: 0.0251 - acc: 0.9989\n",
      "Epoch 246/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9990\n",
      "Epoch 00246: loss did not improve from 0.02310\n",
      "21809/21809 [==============================] - 14s 632us/sample - loss: 0.0233 - acc: 0.9990\n",
      "Epoch 247/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9992\n",
      "Epoch 00247: loss improved from 0.02310 to 0.02112, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 13s 613us/sample - loss: 0.0211 - acc: 0.9992\n",
      "Epoch 248/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9992\n",
      "Epoch 00248: loss improved from 0.02112 to 0.02025, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 627us/sample - loss: 0.0202 - acc: 0.9992\n",
      "Epoch 249/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9990\n",
      "Epoch 00249: loss improved from 0.02025 to 0.01965, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 650us/sample - loss: 0.0196 - acc: 0.9990\n",
      "Epoch 250/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9991\n",
      "Epoch 00250: loss improved from 0.01965 to 0.01870, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 678us/sample - loss: 0.0187 - acc: 0.9991\n",
      "Epoch 251/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9991\n",
      "Epoch 00251: loss improved from 0.01870 to 0.01788, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 632us/sample - loss: 0.0179 - acc: 0.9991\n",
      "Epoch 252/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9992\n",
      "Epoch 00252: loss improved from 0.01788 to 0.01709, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 13s 601us/sample - loss: 0.0171 - acc: 0.9992\n",
      "Epoch 253/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9993\n",
      "Epoch 00253: loss improved from 0.01709 to 0.01589, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 13s 606us/sample - loss: 0.0159 - acc: 0.9993\n",
      "Epoch 254/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9994\n",
      "Epoch 00254: loss improved from 0.01589 to 0.01573, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 13s 598us/sample - loss: 0.0157 - acc: 0.9994\n",
      "Epoch 255/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9993\n",
      "Epoch 00255: loss improved from 0.01573 to 0.01513, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 13s 597us/sample - loss: 0.0151 - acc: 0.9993\n",
      "Epoch 256/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9993\n",
      "Epoch 00256: loss did not improve from 0.01513\n",
      "21809/21809 [==============================] - 13s 596us/sample - loss: 0.0156 - acc: 0.9993\n",
      "Epoch 257/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9991\n",
      "Epoch 00257: loss did not improve from 0.01513\n",
      "21809/21809 [==============================] - 13s 593us/sample - loss: 0.0154 - acc: 0.9991\n",
      "Epoch 258/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9991\n",
      "Epoch 00258: loss improved from 0.01513 to 0.01500, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 13s 592us/sample - loss: 0.0150 - acc: 0.9991\n",
      "Epoch 259/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9990\n",
      "Epoch 00259: loss did not improve from 0.01500\n",
      "21809/21809 [==============================] - 13s 598us/sample - loss: 0.0159 - acc: 0.9990\n",
      "Epoch 260/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 1.0311 - acc: 0.7560\n",
      "Epoch 00260: loss did not improve from 0.01500\n",
      "21809/21809 [==============================] - 13s 591us/sample - loss: 1.0321 - acc: 0.7560\n",
      "Epoch 261/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.6562 - acc: 0.8154\n",
      "Epoch 00261: loss did not improve from 0.01500\n",
      "21809/21809 [==============================] - 13s 608us/sample - loss: 0.6556 - acc: 0.8154\n",
      "Epoch 262/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9391\n",
      "Epoch 00262: loss did not improve from 0.01500\n",
      "21809/21809 [==============================] - 13s 606us/sample - loss: 0.2184 - acc: 0.9392\n",
      "Epoch 263/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9886\n",
      "Epoch 00263: loss did not improve from 0.01500\n",
      "21809/21809 [==============================] - 14s 626us/sample - loss: 0.0717 - acc: 0.9885\n",
      "Epoch 264/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9974\n",
      "Epoch 00264: loss did not improve from 0.01500\n",
      "21809/21809 [==============================] - 14s 631us/sample - loss: 0.0361 - acc: 0.9974\n",
      "Epoch 265/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9990\n",
      "Epoch 00265: loss did not improve from 0.01500\n",
      "21809/21809 [==============================] - 14s 626us/sample - loss: 0.0236 - acc: 0.9990\n",
      "Epoch 266/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9990\n",
      "Epoch 00266: loss did not improve from 0.01500\n",
      "21809/21809 [==============================] - 14s 634us/sample - loss: 0.0202 - acc: 0.9990\n",
      "Epoch 267/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9992\n",
      "Epoch 00267: loss did not improve from 0.01500\n",
      "21809/21809 [==============================] - 14s 620us/sample - loss: 0.0181 - acc: 0.9992\n",
      "Epoch 268/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9992\n",
      "Epoch 00268: loss did not improve from 0.01500\n",
      "21809/21809 [==============================] - 14s 660us/sample - loss: 0.0169 - acc: 0.9992\n",
      "Epoch 269/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9992\n",
      "Epoch 00269: loss did not improve from 0.01500\n",
      "21809/21809 [==============================] - 15s 670us/sample - loss: 0.0159 - acc: 0.9992\n",
      "Epoch 270/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9993\n",
      "Epoch 00270: loss improved from 0.01500 to 0.01493, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 631us/sample - loss: 0.0149 - acc: 0.9993\n",
      "Epoch 271/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9992\n",
      "Epoch 00271: loss improved from 0.01493 to 0.01466, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 647us/sample - loss: 0.0147 - acc: 0.9992\n",
      "Epoch 272/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9993\n",
      "Epoch 00272: loss improved from 0.01466 to 0.01421, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 13s 608us/sample - loss: 0.0142 - acc: 0.9993\n",
      "Epoch 273/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9991\n",
      "Epoch 00273: loss improved from 0.01421 to 0.01378, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 13s 611us/sample - loss: 0.0138 - acc: 0.9991\n",
      "Epoch 274/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9991\n",
      "Epoch 00274: loss improved from 0.01378 to 0.01362, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 622us/sample - loss: 0.0136 - acc: 0.9991\n",
      "Epoch 275/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9991\n",
      "Epoch 00275: loss improved from 0.01362 to 0.01295, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 13s 592us/sample - loss: 0.0129 - acc: 0.9991\n",
      "Epoch 276/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9993\n",
      "Epoch 00276: loss improved from 0.01295 to 0.01220, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 14s 657us/sample - loss: 0.0122 - acc: 0.9993\n",
      "Epoch 277/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9993\n",
      "Epoch 00277: loss did not improve from 0.01220\n",
      "21809/21809 [==============================] - 15s 701us/sample - loss: 0.0122 - acc: 0.9993\n",
      "Epoch 278/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9991\n",
      "Epoch 00278: loss did not improve from 0.01220\n",
      "21809/21809 [==============================] - 14s 645us/sample - loss: 0.0124 - acc: 0.9991\n",
      "Epoch 279/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9990\n",
      "Epoch 00279: loss did not improve from 0.01220\n",
      "21809/21809 [==============================] - 15s 686us/sample - loss: 0.0150 - acc: 0.9989\n",
      "Epoch 280/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.7815 - acc: 0.8031\n",
      "Epoch 00280: loss did not improve from 0.01220\n",
      "21809/21809 [==============================] - 15s 701us/sample - loss: 0.7822 - acc: 0.8031\n",
      "Epoch 281/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.5905 - acc: 0.8331\n",
      "Epoch 00281: loss did not improve from 0.01220\n",
      "21809/21809 [==============================] - 15s 707us/sample - loss: 0.5899 - acc: 0.8333\n",
      "Epoch 282/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9522\n",
      "Epoch 00282: loss did not improve from 0.01220\n",
      "21809/21809 [==============================] - 14s 660us/sample - loss: 0.1694 - acc: 0.9523\n",
      "Epoch 283/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9910\n",
      "Epoch 00283: loss did not improve from 0.01220\n",
      "21809/21809 [==============================] - 15s 668us/sample - loss: 0.0593 - acc: 0.9910\n",
      "Epoch 284/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9981\n",
      "Epoch 00284: loss did not improve from 0.01220\n",
      "21809/21809 [==============================] - 15s 686us/sample - loss: 0.0288 - acc: 0.9981\n",
      "Epoch 285/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9989\n",
      "Epoch 00285: loss did not improve from 0.01220\n",
      "21809/21809 [==============================] - 15s 705us/sample - loss: 0.0194 - acc: 0.9989\n",
      "Epoch 286/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9991\n",
      "Epoch 00286: loss did not improve from 0.01220\n",
      "21809/21809 [==============================] - 16s 743us/sample - loss: 0.0157 - acc: 0.9991\n",
      "Epoch 287/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9993\n",
      "Epoch 00287: loss did not improve from 0.01220\n",
      "21809/21809 [==============================] - 17s 794us/sample - loss: 0.0142 - acc: 0.9993\n",
      "Epoch 288/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9992\n",
      "Epoch 00288: loss did not improve from 0.01220\n",
      "21809/21809 [==============================] - 16s 744us/sample - loss: 0.0135 - acc: 0.9992\n",
      "Epoch 289/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9992\n",
      "Epoch 00289: loss did not improve from 0.01220\n",
      "21809/21809 [==============================] - 14s 644us/sample - loss: 0.0130 - acc: 0.9992\n",
      "Epoch 290/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9993\n",
      "Epoch 00290: loss improved from 0.01220 to 0.01215, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 15s 699us/sample - loss: 0.0122 - acc: 0.9993\n",
      "Epoch 291/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9993\n",
      "Epoch 00291: loss improved from 0.01215 to 0.01146, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 756us/sample - loss: 0.0115 - acc: 0.9993\n",
      "Epoch 292/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9994\n",
      "Epoch 00292: loss improved from 0.01146 to 0.01131, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 16s 717us/sample - loss: 0.0113 - acc: 0.9993\n",
      "Epoch 293/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9992\n",
      "Epoch 00293: loss did not improve from 0.01131\n",
      "21809/21809 [==============================] - 13s 606us/sample - loss: 0.0117 - acc: 0.9992\n",
      "Epoch 294/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9992\n",
      "Epoch 00294: loss did not improve from 0.01131\n",
      "21809/21809 [==============================] - 14s 631us/sample - loss: 0.0114 - acc: 0.9992\n",
      "Epoch 295/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9994\n",
      "Epoch 00295: loss improved from 0.01131 to 0.01021, saving model to ./model_weights.hdf5\n",
      "21809/21809 [==============================] - 13s 619us/sample - loss: 0.0102 - acc: 0.9994\n",
      "Epoch 296/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9992\n",
      "Epoch 00296: loss did not improve from 0.01021\n",
      "21809/21809 [==============================] - 13s 612us/sample - loss: 0.0116 - acc: 0.9992\n",
      "Epoch 297/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9991\n",
      "Epoch 00297: loss did not improve from 0.01021\n",
      "21809/21809 [==============================] - 13s 613us/sample - loss: 0.0109 - acc: 0.9991\n",
      "Epoch 298/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.9097\n",
      "Epoch 00298: loss did not improve from 0.01021\n",
      "21809/21809 [==============================] - 13s 610us/sample - loss: 0.3433 - acc: 0.9093\n",
      "Epoch 299/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.7935 - acc: 0.7882\n",
      "Epoch 00299: loss did not improve from 0.01021\n",
      "21809/21809 [==============================] - 14s 639us/sample - loss: 0.7933 - acc: 0.7883\n",
      "Epoch 300/300\n",
      "21760/21809 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9333\n",
      "Epoch 00300: loss did not improve from 0.01021\n",
      "21809/21809 [==============================] - 14s 626us/sample - loss: 0.2346 - acc: 0.9335\n"
     ]
    }
   ],
   "source": [
    "# Early stopping allows model to stop training if improvement stops.\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "model.compile(optimizer='adam',\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "filepath = \"./model_weights.hdf5\"\n",
    "# Model checkpointing allows us to preserve progress during training if training is interrupted\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "history = model.fit(np.asarray(trainX),\n",
    "         pd.get_dummies(np.asarray(trainy)),\n",
    "         epochs = 300,\n",
    "         batch_size = 128,\n",
    "         callbacks = callbacks_list,\n",
    "         verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3 was trained for 300 epochs and reached .63 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaded after being trained\n",
    "model.load_weights('model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the generation models\n",
    "If this were any other type of project then a good metric to quantify the model's success would be to do a **train-test split to identify the testing accuracy score** using the models to predict data it was not trained on and had never seen before. \n",
    "\n",
    "However, the goal of text generation isn't quite to maximize accuracy, because that would amount to the model regurgitating quotes and would be overfitting. Instead we'll compare the model outputs to the same input strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(model,seq,max_len = 20):\n",
    "    ''' Generates a sequence given a string seq using specified model until the total sequence length\n",
    "    reaches max_len'''\n",
    "    # Tokenize the input string\n",
    "    tokenized_sent = tokenizer.texts_to_sequences([seq])\n",
    "    max_len = max_len+len(tokenized_sent[0])\n",
    "    # If sentence is not as long as the desired sentence length, we need to 'pad sequence' so that\n",
    "    # the array input shape is correct going into our LSTM. the `pad_sequences` function adds \n",
    "    # zeroes to the left side of our sequence until it becomes 19 long, the number of input features.\n",
    "    while len(tokenized_sent[0]) < max_len:\n",
    "        padded_sentence = pad_sequences(tokenized_sent[-19:],maxlen=19)\n",
    "        op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
    "        tokenized_sent[0].append(op.argmax()+1)\n",
    "        \n",
    "    return \" \".join(map(lambda x : reverse_word_map[x],tokenized_sent[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [model]\n",
    "def test_models(test_string,sequence_length= 50, model_list = model_list):\n",
    "    '''Generates output given input test_string up to sequence_length'''\n",
    "    print('Input String: ', test_string)\n",
    "    for counter,model in enumerate(model_list):\n",
    "        print(\"Model \", counter+1, \":\")\n",
    "        print(gen(model,test_string,sequence_length))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String:  \n",
      "Model  1 :\n",
      "is a well characterized pathophysiological mechanism in seizures and proven treatment strategies to address them values 10 hz mice the whole brain and dopaminergic tone by the same datasets that relies on subjective in their here to neurostimulation and fractional stimuli can be 50 150 ms concurrent with electrical stimulation in children with bects this architecture features suggesting with a pediatric receptor suppressed m current in the axons functional tissue is a high positive validation from spindles this detection approach that analyze candidate parameters in humans epilepsy is a common childhood stimulation parameters identified we will determine whether stimulation impacts\n"
     ]
    }
   ],
   "source": [
    "test_models('', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
